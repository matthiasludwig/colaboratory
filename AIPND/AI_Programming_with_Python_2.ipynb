{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "To calculate the probability that a prediction is of one of many classes:\n",
    "\n",
    "e^Z_i / e^Z_1 ... e^Z_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    result = 0.0\n",
    "    for y, p in zip(Y, P):\n",
    "        if y == 1:\n",
    "            result += np.log(p)\n",
    "        if y == 0:\n",
    "            result += np.log(1-p)\n",
    "    \n",
    "    return (result * (-1))\n",
    "\n",
    "print(cross_entropy([1,0,1,1], [0.4,0.6,0.1,0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "* Error Funciton: E=−yln(y^​)−(1−y)ln(1−y^​)\n",
    "* Gradient Descent Step: wi′​←wi​+α(y−y^​)xi​. AND b′←b+α(y−y^​),\n",
    "\n",
    "Pseudo-code for the Gradient Descent Step:\n",
    "\n",
    "1. Initialize W and b with random values\n",
    "2. For every point (x1,...,xn)\n",
    "2.1 For i=1...n\n",
    "2.1.1 Update wi​+α(y−y^​)xi\n",
    "2.1.2 Update b+α(y−y^​)\n",
    "3. Repeat until the error is small (Epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the weight step to zero: Δwi=0\\Delta w_i = 0Δwi​=0\n",
    "For each record in the training data:\n",
    "\n",
    "    * Make a forward pass through the network, calculating the output y^=f(∑iwixi)\\hat y = f(\\sum_i w_i x_i)y^​=f(∑i​wi​xi​)\n",
    "    * Calculate the error term for the output unit, δ=(y−y^)∗f′(∑iwixi)\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)δ=(y−y^​)∗f′(∑i​wi​xi​)\n",
    "    * Update the weight step Δwi=Δwi+δxi\\Delta w_i = \\Delta w_i + \\delta x_iΔwi​=Δwi​+δxi​\n",
    "\n",
    "Update the weights wi=wi+ηΔwi/mw_i = w_i + \\eta \\Delta w_i / mwi​=wi​+ηΔwi​/m where η\\etaη is the learning rate and mmm is the number of records.\n",
    "\n",
    "Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "\n",
    "Repeat for e epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n[0.00804047 0.00555918]\nChange in weights for input layer to hidden layer:\n[[ 1.77005547e-04 -5.11178506e-04]\n [ 3.54011093e-05 -1.02235701e-04]\n [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = weights_hidden_output * output_error_term * hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:,None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
