{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Nanodegree.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ym9D8-P3jJCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introductions"
      ]
    },
    {
      "metadata": {
        "id": "Ahm0DG1cyqdO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Deep Learning Foundation Program is divided into **six chapters**, some of which including a project to complete:\n",
        "\n",
        "\n",
        "1.   Introductions\n",
        "2.   Neural Networks (**P1: Your first neural network**)\n",
        "3.   Convolutional Neural Networks (**P2: Dog Breed Classifier**)\n",
        "4.   Recurrent Neural Networks (**P3: Generate TV Scripts**)\n",
        "5.   Generative Adversarial Networks (**P4: Generate Faces**)\n",
        "6.   Deep Reinforcement Learning (**P5: Teach a Quadcopter to Fly**)\n",
        "\n",
        "**Machine Learning:**\n",
        "* Supervised (Feedback)\n",
        "* Unsupervised (No labels)\n",
        "* Reinforcement Learning (Feedback at the end)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Anaconda\n",
        "Conda is a package manager that organizes dependencies in environments\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "conda create -n name packages=3       # Creates a new environment\n",
        "conda env list                        # Lists all environments\n",
        "source activate name                  # Activates an environment\n",
        "  conda install numpy pandas ...      # Installs packages inside the environment\n",
        "  conda list                          # Lists all packages installed in the environment\n",
        "  conda env export > environment.yaml # Exports all depencencies into a yaml file\n",
        "conda env create -f environment.yaml  # Creates a new environment from the yaml file\n",
        "conda env remove -n name              # Remove conda env\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "M8b5SqkDBCsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Jupyter\n",
        "A notebook is a web appplication that allows you to combine explanatory text, math equations, code and visualizations all in one easily sharable document.\n",
        "It is an example of [literate programming](http://www.literateprogramming.com/)\n",
        "\n",
        "### How Jupyter works:\n",
        "\n",
        "![explanation of how jupyter notebooks work](https://jupyter.readthedocs.io/en/latest/_images/notebook_components.png)"
      ]
    },
    {
      "metadata": {
        "id": "tE-d5FOvBLu0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Matrix Math and Numpy Refresher\n",
        "### Data Dimensions\n",
        "**Scalar:** Simplest shape | 0 dimsension\n",
        "**Vectors:** Row or Column vectors | 1 dimesion = length\n",
        "**Matrices:** 2d Vector | 2 dimensions\n",
        "**Tensors:** n-dimensional tensor\n",
        "\n",
        "Axy = x is the row and y is the column.\n",
        "\n",
        "### Numpy\n",
        "Written in C to perform fast mathematical operations."
      ]
    },
    {
      "metadata": {
        "id": "XMeaagt28Z-A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "946279e7-6f3c-45e8-b6d8-5a8595db2e68",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527792445820,
          "user_tz": -120,
          "elapsed": 910,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "s = np.array(5)        # Scalar\n",
        "s.shape                # () since it is a scalar\n",
        "\n",
        "v = np.array([1,2,3])  # Vector\n",
        "v.shape                # (3,)\n",
        "v[1]                   # 2\n",
        "v[1:]                  # 2,3 | Access elements from the second one on\n",
        "\n",
        "m = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
        "m.shape                # (3,3)\n",
        "\n",
        "t = np.array([[[[1],[2]],[[3],[4]],[[5],[6]]],[[[7],[8]],[[9],[10]],[[11],[12]]],[[[13],[14]],[[15],[16]],[[17],[17]]]])\n",
        "t.shape                # (3,3,2,1)\n",
        "\n",
        "v.reshape(1,3)         # Change the shape\n",
        "v = v[:, None]         # More experienced reshaping\n",
        "v = v[None,:]\n",
        "v"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1],\n",
              "        [2],\n",
              "        [3]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "GQ-MnqWR-33Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Element-wise Matrix Operations\n",
        "Treat the items in the matrix individually and perform the same operation on each one.\n",
        "Matrices have to have the same shape."
      ]
    },
    {
      "metadata": {
        "id": "j63fjn-u9Fdg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bab5fc50-1cc7-404b-f50c-e44166d0fc79",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523904757908,
          "user_tz": -120,
          "elapsed": 624,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "values = [1,2,3,4,5]\n",
        "values = np.array(values) + 5\n",
        "print(values)            # [6,7,8,9,10]\n",
        "\n",
        "values = np.multiply(values, 5)\n",
        "print(values)            # [30,35,40,50]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6  7  8  9 10]\n",
            "[30 35 40 45 50]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jWBGvQVEwFle",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication\n",
        "Matrices don't have to have the same shape.\n",
        "Rows of the first Matrix and the columns of the second matrix. (Taking the Dot Product multiple times)\n",
        "\n",
        "#### Import Reminders about Matrix Multiplication\n",
        "* Number of columns in the left matrix must be equal to the number of rows in the second matrix. (2x3 and 3x2) (RowsXColumns)\n",
        "* The Result will have the same number of rows as the left matrix and the same number of columns as the right matrix.\n",
        "* Order matters. A*B != B*A\n",
        "* The data in the left should be ordered in rows and in columns in the left\n",
        "\n",
        "#### Dot Product\n",
        "Multiply the corresponding elements of each vector. Then we add up all the results."
      ]
    },
    {
      "metadata": {
        "id": "vu6RHdibyg5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NumPy Matrix Multiplication\n",
        "\n",
        "**Element-wise** Multiplication: Using the * or multiply function:"
      ]
    },
    {
      "metadata": {
        "id": "MAYmkPD1zI5Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4dc4170a-42a8-40e6-f109-2fe5ee34615e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525722900028,
          "user_tz": -120,
          "elapsed": 654,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "multiple = np.array([[1,2,3],[4,5,6]])\n",
        "multiple\n",
        "\n",
        "new = multiple * 0.25\n",
        "new\n",
        "\n",
        "new * multiple\n",
        "\n",
        "np.multiply(multiple, new)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25, 1.  , 2.25],\n",
              "       [4.  , 6.25, 9.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "gxXL22HE0xCa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finding the **Matrix Product** using NumPy's matmul function:"
      ]
    },
    {
      "metadata": {
        "id": "7UmhIFVi03za",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "204ff9de-1b4c-4116-e173-b87ac5c2cf45",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523905467098,
          "user_tz": -120,
          "elapsed": 644,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.array([[1,2,3,4],[5,6,7,8]])\n",
        "a\n",
        "\n",
        "a.shape # (2,4)\n",
        "\n",
        "b = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
        "b\n",
        "\n",
        "b.shape # (4,3)\n",
        "\n",
        "c = np.matmul(a,b)\n",
        "c\n",
        "\n",
        "c.shape # 2,3"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "guqggPPA2KLU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "NumPy's **dot function** can be identical to matmul (if the matrices are 2-dimensional)"
      ]
    },
    {
      "metadata": {
        "id": "YNgNPNR92ZA6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2ff419d2-188f-433f-8424-ded97334ed49",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523905775476,
          "user_tz": -120,
          "elapsed": 416,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.array([[1,2],[3,4]])\n",
        "\n",
        "np.dot(a,a)\n",
        "a.dot(a)\n",
        "np.matmul(a,a)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7, 10],\n",
              "       [15, 22]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "nDmenn7-5rsS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transpose\n",
        "\n",
        "If the original Matrix was **not** a square the new transpose will have the dimensions swapped.\n",
        "Each feature is either in a row or a column.\n",
        "\n",
        "The option if you should transpose depends on the situation(!)\n",
        "\n",
        "**The only time you can safely use a tranpose if both data is arranged as rows**"
      ]
    },
    {
      "metadata": {
        "id": "pT3CyPSx7BOG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "519ba9a9-52ef-48f5-d182-f38c343ad8c4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523906814802,
          "user_tz": -120,
          "elapsed": 692,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "m = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
        "m\n",
        "\n",
        "m.T # Transpose of m"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  9],\n",
              "       [ 2,  6, 10],\n",
              "       [ 3,  7, 11],\n",
              "       [ 4,  8, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "svSYT5JKA07m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "yb6Q8vXOCq8K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 1: Introduction to Neural Networks\n",
        "\n",
        "First idea is to compare Neural Networks to Linear Regression.\n",
        "\n",
        "W = Weights\n",
        "b = Bias\n",
        "x = Input\n",
        "y = label\n",
        "ÿ = Prediction\n",
        "\n",
        "### Perceptron\n",
        "\n",
        "**Comparison:** The Perceptron is the 2\\*Test + 1\\*Grades -18. Smaller Nodes are Inputs. Arrows are the Weights/Bias.\n",
        "\n",
        "![Image Perceptron](https://)\n",
        "\n",
        "#### Perceptron as Logical Operators\n",
        "\n",
        "**AND:** Only true if both INs are 1\n",
        "\n",
        "**OR:** If any of it's INs are 1\n",
        "\n",
        "**NOT:** Flips one of the Inputs\n",
        "\n",
        "**NAND:** Not and and\n",
        "\n",
        "The Perceptron Steps (Only works for linear function):\n"
      ]
    },
    {
      "metadata": {
        "id": "mhTfPfMlpc-S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
        "    # Fill in code\n",
        "    for i in range(len(X)):\n",
        "        pred = prediction(X[i], W, b)\n",
        "        if y[i] - pred == 1:\n",
        "            W[0] += X[i][0] * learn_rate\n",
        "            W[1] += X[i][1] * learn_rate\n",
        "            b += learn_rate\n",
        "        if y[i] - pred == -1:\n",
        "            W[0] -= X[i][0] * learn_rate\n",
        "            W[1] -= X[i][1] * learn_rate\n",
        "            b -= learn_rate\n",
        "    return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRu6T9Ptp6xq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Error Functions\n",
        "\n",
        "* Minimizing the Error functions => Avoid local minimum. (Gradient Descent)\n",
        "* Error functions needs to be differentiable and continuous.\n",
        "\n",
        "**Goal:** Finding the way to minimize the Loss the fastest. The challenge is not to get stuck in a local minimum.\n",
        "Correctly classified points should carry a small Error. Not correctly classified points should carry a high penalty.\n",
        "\n",
        "**Prediction:** Discrete would be: Yes/no. Continious would be 0-100% likelyhood of something:\n",
        "* **Sigmoid:** Change the Activation function from Step function to sigmoid. 1/(1+e^-x)\n",
        "* **Softmax:** Classification Problems. The probabilities across all options have to add to 1. e^Zi / e^Zn\n",
        "* **One-Hot Encoding:** Inputvariables. One variable for each class. Each class has a column with either 0 or 1. So the value for each element is for example [0,0,1]"
      ]
    },
    {
      "metadata": {
        "id": "Sf1_z6Sl2qu2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Maximum Probability\n",
        "\n",
        "All Probabilities of each points multiplied. Gives the Probabilities that these points are the respective colors. The goal then becomes to maximize the likelyhood.\n",
        "* Increasing the probability => Deceasing the error\n",
        "* Going from products to sums: log. Becasue log(ab) = log(a) + log(b). All negative numbers => -ln(x)"
      ]
    },
    {
      "metadata": {
        "id": "s2XsTXpZ2t3A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy\n",
        "\n",
        "=> -ln(x).\n",
        "\n",
        "Good model gives a small Cross Entropy, A bad model gives a high Cross Entropy.\n",
        "\n",
        "Cross entropy says if a bunch of events and a bunch of probabilities, how likely is it that those events happen based on the probabilities?\n",
        "\n",
        "* Very likely: Small CE\n",
        "* Very unlikely: Big CE\n",
        "\n",
        "All probabilities of all Ys adds up to 1.\n",
        "\n",
        "**CE:** Gives how similiar two vectors are."
      ]
    },
    {
      "metadata": {
        "id": "I4uJZ0kE2CvG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c718294-a8c8-494d-e5dc-8ec3745cfc5c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526585265920,
          "user_tz": -120,
          "elapsed": 708,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "L = [1,0,1,1]\n",
        "P = [0.4,0.6,0.1,0.5]\n",
        "\n",
        "# Write a function that takes as input two lists Y, P,\n",
        "# and returns the float corresponding to their cross-entropy.\n",
        "def cross_entropy(Y, P):\n",
        "    \n",
        "    result = 0\n",
        "    \n",
        "    for y,p in zip(Y,P):\n",
        "        if y == 0:\n",
        "            result += np.log(1 - p)\n",
        "        else:\n",
        "            result += np.log(p)\n",
        "    \n",
        "    return result * (-1)\n",
        "  \n",
        "print(cross_entropy(L,P))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.828313737302301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "twa_vpQp2bxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-Class Cross Entropy\n",
        "\n",
        "**CE:** -Ei Ej yij ln(pij)\n",
        "\n",
        "A higher CE correlates to a lower Probability of this event happening (and vice versa)"
      ]
    },
    {
      "metadata": {
        "id": "VAar5B9P3hMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent (Step)\n",
        "\n",
        "From the Error function through Gradient Descent a smaller Error function is searched for.\n",
        "\n",
        "Negative of the Gradient of the Error function leads us to the fastest way *\"down\"*. (Times a small learning rate)\n",
        "\n",
        "The **Derivative of the Sigmoid function** is: s(x) * (1-s(x))\n",
        "\n",
        "The **Derivative of the Error function** is: -(y - ÿ)(x1,...,xn,1)\n",
        "\n",
        "#### Step\n",
        "\n",
        "w'i <- wi + alpha(y-ÿ)xi (where alpha is the learning rate **and** 1/m * alpha)\n",
        "\n",
        "The update of the bias is similiar.\n",
        "\n",
        "\n",
        "#### Pseudo Code\n",
        "\n",
        "1. Start with random weights\n",
        "2. For every point\n",
        "\n",
        "  2.1 For i = i...n\n",
        "  \n",
        "    2.1.1 Update w'i <- wi - alpha(ÿ-y)xi\n",
        "    \n",
        "    2.2.2 Update b' <- b - alpha(ÿ-y)\n",
        "3. Repeat until the error is small\n",
        "\n",
        "(How many times = epochs)"
      ]
    },
    {
      "metadata": {
        "id": "3z7Q45NvDJp0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def output_formula(features, weights, bias):\n",
        "    return sigmoid(np.dot(features, weights) + bias)\n",
        "\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
        "\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    output = output_formula(x, weights, bias)\n",
        "    d_error = -(y - output)\n",
        "    weights -= learnrate * d_error * x\n",
        "    bias -= learnrate * d_error\n",
        "    return weights, bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRXWZOU4HMhC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network Architecture\n",
        "\n",
        "**Non-linear Models:** The line seperating two point groups will not be linear anymore this is where Neural Networks come into play\n",
        "\n",
        "=> Combining two linear models. Two probabilies via the sigmoid function. Adding two linear models to obtain a third model.\n",
        "\n",
        "* Input Layer (containts the inputs x1, x2,..., xn)\n",
        "* Hidden Layer\n",
        "* Output Layer (non-linear space)\n",
        "\n",
        "**Multi-class problems** have more Output Layers\n",
        "\n",
        "**More Hidden Layers:** Deep(er) Neural Network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RpVaQDWmb6-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feedforward\n",
        "\n",
        "This is the process of neural networks use to turn the input into an output.\n",
        "Through the weights the different inputs get a stronger emphasis.\n",
        "\n",
        "Input vector -> Apply a sequence of linear functions and sigmoid functions to get a highly non-linear output layer."
      ]
    },
    {
      "metadata": {
        "id": "YBX5q9wLdEi4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "\n",
        "E(W) = 1/m E yi ln(ÿi) + (1 - yi)ln(1 - ÿi)\n",
        "\n",
        "* Doing a feedforward operation\n",
        "* Comparing the output to the desired output (y - ÿ)\n",
        "* Calculating the Error\n",
        "* Running the feedforward operation backwards to spread the error to each of the weights\n",
        "* Use this to update the weights and get a better model\n",
        "* Repeat this process\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NxmBDCO5hWXG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 2: Implementing Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "CmLp-Tx35x97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the last chapter we used the log-loss function. There are many other error functions used for neural networks. One is called the mean squared error: It is the mean of the squares of the differences between the predictions and the labels.\n",
        "\n",
        "**SSE:** E = 1/2 EE (yj - ÿj)^2\n",
        "\n",
        "**Caveats:** We can end up in a local minima. That happens if the weights are initialized with the wrong values.\n",
        "\n",
        "The **derivative of the Error** with respect to wi is: - (y - ÿ)f'(h)xi"
      ]
    },
    {
      "metadata": {
        "id": "fA4kOF9It1Xm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1e8869f-0e56-4fbe-c94b-999e1d69a2f7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526848713863,
          "user_tz": -120,
          "elapsed": 991,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Defining the sigmoid function for activations\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Input data\n",
        "x = np.array([0.1, 0.3])\n",
        "# Target\n",
        "y = 0.2\n",
        "# Input to output weights\n",
        "weights = np.array([-0.8, 0.5])\n",
        "\n",
        "# The learning rate, eta in the weight step equation\n",
        "learnrate = 0.5\n",
        "\n",
        "# the linear combination performed by the node (h in f(h) and f'(h))\n",
        "h = x[0]*weights[0] + x[1]*weights[1]\n",
        "# or h = np.dot(x, weights)\n",
        "\n",
        "# The neural network output (y-hat)\n",
        "nn_output = sigmoid(h)\n",
        "\n",
        "# output error (y - y-hat)\n",
        "error = y - nn_output\n",
        "\n",
        "# output gradient (f'(h))\n",
        "output_grad = sigmoid_prime(h)\n",
        "\n",
        "# error term (lowercase delta)\n",
        "error_term = error * output_grad\n",
        "\n",
        "# Gradient descent step \n",
        "del_w = [ learnrate * error_term * x[0],\n",
        "          learnrate * error_term * x[1]]\n",
        "# or del_w = learnrate * error_term * x\n",
        "\n",
        "print(del_w)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.003963803079006883, -0.011891409237020648]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B_cGkjazvy-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's the general algorithm for updating the weights with gradient descent:\n",
        "\n",
        "Set the weight step to zero: Δwi=0\\Delta w_i = 0Δwi​=0\n",
        "\n",
        "**For each record in the training data:**\n",
        "\n",
        "* Make a forward pass through the network, calculating the output y^=f(∑iwixi)\\hat y = f(\\sum_i w_i x_i)y^​=f(∑i​wi​xi​)\n",
        "* Calculate the error term for the output unit, δ=(y−y^)∗f′(∑iwixi)\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)δ=(y−y^​)∗f′(∑i​wi​xi​)\n",
        "* Update the weight step Δwi=Δwi+δxi\\Delta w_i = \\Delta w_i + \\delta x_iΔwi​=Δwi​+δxi​\n",
        "* Update the weights wi=wi+ηΔwi/mw_i = w_i + \\eta \\Delta w_i / mwi​=wi​+ηΔwi​/m where η\\etaη is the learning rate and mmm is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
        "\n",
        "Repeat for e epochs.\n",
        "\n",
        "**Example with Epochs:**\n"
      ]
    },
    {
      "metadata": {
        "id": "hr1cB50D0UZj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
        "#       the previous lesson to encourage you to come up with a more\n",
        "#       efficient solution. If you need a hint, check out the comments\n",
        "#       in solution.py from the previous lecture.\n",
        "\n",
        "# Use to same seed to make debugging easier\n",
        "np.random.seed(42)\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w = np.zeros(weights.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        # Loop through all records, x is the input, y is the target\n",
        "\n",
        "        # Note: We haven't included the h variable from the previous\n",
        "        #       lesson. You can add it if you want, or you can calculate\n",
        "        #       the h together with the output\n",
        "\n",
        "        # TODO: Calculate the output\n",
        "        h = np.dot(x, weights)\n",
        "        \n",
        "        output = sigmoid(h)\n",
        "\n",
        "        # TODO: Calculate the error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: Calculate the error term\n",
        "        #output_grad = output * (1 - output)\n",
        "        \n",
        "        error_term = error * output * (1 - output)\n",
        "\n",
        "        # TODO: Calculate the change in weights for this sample\n",
        "        #       and add it to the total weight change\n",
        "        del_w += error_term * x\n",
        "\n",
        "    # TODO: Update weights using the learning rate and the average change in weights\n",
        "    weights += learnrate * del_w / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        out = sigmoid(np.dot(features, weights))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "tes_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = tes_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "naiGag7S10mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multilayer Preceptrons\n",
        "\n",
        "With hidden units, the weights between them will require two incides: wij, where i denotes input units and j are the hidden units\n",
        "\n",
        "#### Making a column vector\n",
        "\n",
        "It's possible to get the transpose of an array like so arr.T but for a 1D array, the transpose will return a row vector. Instead use arr[:,None] to create a column vector:"
      ]
    },
    {
      "metadata": {
        "id": "utxxEpMm56O2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f02e92ef-be94-4e98-92b9-95151e6af3b7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526154733565,
          "user_tz": -120,
          "elapsed": 567,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "features = np.array([0.49671415, -0.1382643 , 0.64768854])\n",
        "\n",
        "print(features)\n",
        "# array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
        "\n",
        "print(features.T)\n",
        "# array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
        "\n",
        "print(features[:, None])\n",
        "# array([[ 0.49671415],\n",
        "#       [-0.1382643 ],\n",
        "#       [ 0.64768854]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.49671415 -0.1382643   0.64768854]\n",
            "[ 0.49671415 -0.1382643   0.64768854]\n",
            "[[ 0.49671415]\n",
            " [-0.1382643 ]\n",
            " [ 0.64768854]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X2lCWzD86701",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Backpropagation\n",
        "\n",
        "Using the chain rule to find the error with respect to the weights connecting the input layer to the hidden layer (for a two layer network)."
      ]
    },
    {
      "metadata": {
        "id": "WH9KRveU_a77",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d4231c30-2d07-46c3-8647-17682c13ce3a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526895167414,
          "user_tz": -120,
          "elapsed": 664,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "x = np.array([0.5, 0.1, -0.2])\n",
        "target = 0.6\n",
        "learnrate = 0.5\n",
        "\n",
        "weights_input_hidden = np.array([[0.5, -0.6],\n",
        "                                 [0.1, -0.2],\n",
        "                                 [0.1, 0.7]])\n",
        "\n",
        "weights_hidden_output = np.array([0.1, -0.3])\n",
        "\n",
        "## Forward pass\n",
        "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "output = sigmoid(output_layer_in)\n",
        "\n",
        "## Backwards pass\n",
        "## TODO: Calculate output error\n",
        "error = target - output\n",
        "\n",
        "# TODO: Calculate error term for output layer\n",
        "output_error_term = error * output * (1 - output)\n",
        "\n",
        "# TODO: Calculate error term for hidden layer\n",
        "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * hidden_layer_output * (1 - hidden_layer_output)\n",
        "\n",
        "# TODO: Calculate change in weights for hidden layer to output layer\n",
        "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
        "\n",
        "# TODO: Calculate change in weights for input layer to hidden layer\n",
        "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
        "\n",
        "print('Change in weights for hidden layer to output layer:')\n",
        "print(delta_w_h_o)\n",
        "print('Change in weights for input layer to hidden layer:')\n",
        "print(delta_w_i_h)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Change in weights for hidden layer to output layer:\n",
            "[0.00804047 0.00555918]\n",
            "Change in weights for input layer to hidden layer:\n",
            "[[ 1.77005547e-04 -5.11178506e-04]\n",
            " [ 3.54011093e-05 -1.02235701e-04]\n",
            " [-7.08022187e-05  2.04471402e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pYMPMva5BpDo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Implementing Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "Y9z6AT5GBsuL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "03032ef5-7bdb-43ad-f62b-f086e0713470",
        "executionInfo": {
          "status": "error",
          "timestamp": 1526156729701,
          "user_tz": -120,
          "elapsed": 877,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "np.random.seed(21)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_hidden = 2  # number of hidden units\n",
        "epochs = 900\n",
        "learnrate = 0.005\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "# Initialize weights\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        ## Forward pass ##\n",
        "        # TODO: Calculate the output\n",
        "        hidden_input = np.dot(x, weights_input_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "        \n",
        "        input_for_output = np.dot(hidden_output, weights_hidden_output)\n",
        "        \n",
        "        output = sigmoid(input_for_output)\n",
        "\n",
        "        ## Backward pass ##\n",
        "        # TODO: Calculate the network's prediction error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: Calculate error term for the output unit\n",
        "        output_error_term = error * output * (1 - output)\n",
        "\n",
        "        ## propagate errors to hidden layer\n",
        "\n",
        "        # TODO: Calculate the hidden layer's contribution to the error\n",
        "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
        "        \n",
        "        # TODO: Calculate the error term for the hidden layer\n",
        "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
        "        \n",
        "        # TODO: Update the change in weights\n",
        "        del_w_hidden_output += learnrate * output_error_term * hidden_output\n",
        "        del_w_input_hidden += learnrate * hidden_error_term * x[:, None]\n",
        "\n",
        "    # TODO: Update weights\n",
        "    weights_input_hidden += del_w_input_hidden\n",
        "    weights_hidden_output += del_w_hidden_output\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-50c4f398dc71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_prep\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_prep'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "U_hxrg9NB9T8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 3: Training Neural Networks\n",
        "\n",
        "**Training set:** Model is trained with that without looking at the Testing set.\n",
        "\n",
        "**Validation set:** Set of data that is re-introduced at the end of each Epoch to measure the validation loss.\n",
        "\n",
        "**Testing set:** Testing the Model at the end (Re- Introduced)\n",
        "\n",
        "---\n",
        "\n",
        "**Overfitting:** Fit the data well, but it can not generalize. Error due to variance.\n",
        "\n",
        "**Underfitting:** Trying to kill godzilla with a flyswatter. Error due to bias.\n",
        "\n",
        "Both problems also correlate with a too simple/too complicated neural network architecture.\n",
        "We err on the side of an overly complicated model and then apply techniques to prevent overfitting.\n",
        "\n",
        "#### Early Stopping\n",
        "\n",
        "Underfitting: Training and Test errors are big\n",
        "Overfitting: Training Error is tiny and Testing Error Large\n",
        "\n",
        "--> Model Complexity Graph\n",
        "Testing Error increases with more Epochs\n",
        "Training Error decreases with the Epochs\n",
        "We need the point where the **Testing Error increases again** (Early Stopping)"
      ]
    },
    {
      "metadata": {
        "id": "uR09B3BoIxCQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Regularization\n",
        "\n",
        "When we apply sigmoid to 10x1 and 10x2 the function becomes steeper => Gradient Descent becomes harder.\n",
        "\n",
        "##### Punish large coefficients:\n",
        "**L1**: MSE + lamda * w1,...,wn (absolute values)\n",
        "* Good for features selection\n",
        "* Sparse Vectors. Small number of weights.\n",
        "\n",
        "**L2**: MSE + lamda * w1,...,wn (squared)\n",
        "* Normally better for Training Models\n",
        "* Does not favor small vectors"
      ]
    },
    {
      "metadata": {
        "id": "-mcqOyTIK61L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "\n",
        "One part of the network has large weights and dominates the training.\n",
        "**Sometimes during training some nodes are turned off.**\n",
        "Probability that each node will be dropped."
      ]
    },
    {
      "metadata": {
        "id": "dtOsW_6CMBNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vanishing Gradient\n",
        "\n",
        "**Hyperbolic Tangent Function:** e^x - e^x / e^x + e^x\n",
        "\n",
        "**Rectified Linear Unit (ReLU):** If Positive return x. If negative return 0."
      ]
    },
    {
      "metadata": {
        "id": "b3tqj2erM-oF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch vs. Stochastic Gradient Descent\n",
        "\n",
        "Number of steps in the gradient descent = Number of epochs.\n",
        "Epoch whole forward and backward pass for the data.\n",
        "\n",
        "**Stochastic Gradient Descent:** Small subset of the data. Batches of the whole data."
      ]
    },
    {
      "metadata": {
        "id": "nagicM9tOwWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning Rate Decay\n",
        "\n",
        "Best: If steep: Long steps. If plaing: small steps.\n",
        "\n",
        "**Random Restart**: Gradient Descent from different locations.\n",
        "\n",
        "**Momentum**: Average of the last # steps. Beta between 0-1. Step(n) + b*Step(n-1) + ..."
      ]
    },
    {
      "metadata": {
        "id": "8Tuzf828KQI_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 6: Sentiment Analysis\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "l_cVM54HIwQ_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# Encapsulate our neural network in a class\n",
        "class SentimentNetwork:\n",
        "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
        "        \"\"\"Create a SentimenNetwork with the given settings\n",
        "        Args:\n",
        "            reviews(list) - List of reviews used for training\n",
        "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
        "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
        "            learning_rate(float) - Learning rate to use while training\n",
        "        \n",
        "        \"\"\"\n",
        "        # Assign a seed to our random number generator to ensure we get\n",
        "        # reproducable results during development \n",
        "        np.random.seed(1)\n",
        "\n",
        "        # process the reviews and their associated labels so that everything\n",
        "        # is ready for training\n",
        "        self.pre_process_data(reviews, labels)\n",
        "        \n",
        "        # Build the network to have the number of hidden nodes and the learning rate that\n",
        "        # were passed into this initializer. Make the same number of input nodes as\n",
        "        # there are vocabulary words and create a single output node.\n",
        "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
        "\n",
        "    def pre_process_data(self, reviews, labels):\n",
        "        \n",
        "        # populate review_vocab with all of the words in the given reviews\n",
        "        review_vocab = set()\n",
        "        for review in reviews:\n",
        "            for word in review.split(\" \"):\n",
        "                review_vocab.add(word)\n",
        "\n",
        "        # Convert the vocabulary set to a list so we can access words via indices\n",
        "        self.review_vocab = list(review_vocab)\n",
        "        \n",
        "        # populate label_vocab with all of the words in the given labels.\n",
        "        label_vocab = set()\n",
        "        for label in labels:\n",
        "            label_vocab.add(label)\n",
        "        \n",
        "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
        "        self.label_vocab = list(label_vocab)\n",
        "        \n",
        "        # Store the sizes of the review and label vocabularies.\n",
        "        self.review_vocab_size = len(self.review_vocab)\n",
        "        self.label_vocab_size = len(self.label_vocab)\n",
        "        \n",
        "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
        "        self.word2index = {}\n",
        "        for i, word in enumerate(self.review_vocab):\n",
        "            self.word2index[word] = i\n",
        "        \n",
        "        # Create a dictionary of labels mapped to index positions\n",
        "        self.label2index = {}\n",
        "        for i, label in enumerate(self.label_vocab):\n",
        "            self.label2index[label] = i\n",
        "        \n",
        "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Store the learning rate\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights\n",
        "\n",
        "        # These are the weights between the input layer and the hidden layer.\n",
        "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
        "    \n",
        "        # These are the weights between the hidden layer and the output layer.\n",
        "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
        "                                                (self.hidden_nodes, self.output_nodes))\n",
        "        \n",
        "        # The input layer, a two-dimensional matrix with shape 1 x input_nodes\n",
        "        self.layer_0 = np.zeros((1,input_nodes))\n",
        "    \n",
        "    def update_input_layer(self,review):\n",
        "\n",
        "        # clear out previous state, reset the layer to be all 0s\n",
        "        self.layer_0 *= 0\n",
        "        \n",
        "        for word in review.split(\" \"):\n",
        "            # NOTE: This if-check was not in the version of this method created in Project 2,\n",
        "            #       and it appears in Andrew's Project 3 solution without explanation. \n",
        "            #       It simply ensures the word is actually a key in word2index before\n",
        "            #       accessing it, which is important because accessing an invalid key\n",
        "            #       with raise an exception in Python. This allows us to ignore unknown\n",
        "            #       words encountered in new reviews.\n",
        "            if(word in self.word2index.keys()):\n",
        "                self.layer_0[0][self.word2index[word]] += 1\n",
        "                \n",
        "    def get_target_for_label(self,label):\n",
        "        if(label == 'POSITIVE'):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def sigmoid(self,x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def sigmoid_output_2_derivative(self,output):\n",
        "        return output * (1 - output)\n",
        "    \n",
        "    def train(self, training_reviews, training_labels):\n",
        "        \n",
        "        # make sure out we have a matching number of reviews and labels\n",
        "        assert(len(training_reviews) == len(training_labels))\n",
        "        \n",
        "        # Keep track of correct predictions to display accuracy during training \n",
        "        correct_so_far = 0\n",
        "\n",
        "        # Remember when we started for printing time statistics\n",
        "        start = time.time()\n",
        "        \n",
        "        # loop through all the given reviews and run a forward and backward pass,\n",
        "        # updating weights for every item\n",
        "        for i in range(len(training_reviews)):\n",
        "            \n",
        "            # Get the next review and its correct label\n",
        "            review = training_reviews[i]\n",
        "            label = training_labels[i]\n",
        "            \n",
        "            #### Implement the forward pass here ####\n",
        "            ### Forward pass ###\n",
        "\n",
        "            # Input Layer\n",
        "            self.update_input_layer(review)\n",
        "\n",
        "            # Hidden layer\n",
        "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "            # Output layer\n",
        "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "            \n",
        "            #### Implement the backward pass here ####\n",
        "            ### Backward pass ###\n",
        "\n",
        "            # Output error\n",
        "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
        "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
        "\n",
        "            # Backpropagated error\n",
        "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
        "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
        "\n",
        "            # Update the weights\n",
        "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
        "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
        "\n",
        "            # Keep track of correct predictions.\n",
        "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
        "                correct_so_far += 1\n",
        "            elif(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
        "                correct_so_far += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the training process. \n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
        "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
        "            if(i % 2500 == 0):\n",
        "                print(\"\")\n",
        "    \n",
        "    def test(self, testing_reviews, testing_labels):\n",
        "        \"\"\"\n",
        "        Attempts to predict the labels for the given testing_reviews,\n",
        "        and uses the test_labels to calculate the accuracy of those predictions.\n",
        "        \"\"\"\n",
        "        \n",
        "        # keep track of how many correct predictions we make\n",
        "        correct = 0\n",
        "\n",
        "        # we'll time how many predictions per second we make\n",
        "        start = time.time()\n",
        "\n",
        "        # Loop through each of the given reviews and call run to predict\n",
        "        # its label. \n",
        "        for i in range(len(testing_reviews)):\n",
        "            pred = self.run(testing_reviews[i])\n",
        "            if(pred == testing_labels[i]):\n",
        "                correct += 1\n",
        "            \n",
        "            # For debug purposes, print out our prediction accuracy and speed \n",
        "            # throughout the prediction process. \n",
        "\n",
        "            elapsed_time = float(time.time() - start)\n",
        "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
        "            \n",
        "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
        "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
        "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
        "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
        "    \n",
        "    def run(self, review):\n",
        "        \"\"\"\n",
        "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
        "        \"\"\"\n",
        "        # Run a forward pass through the network, like in the \"train\" function.\n",
        "        \n",
        "        # Input Layer\n",
        "        self.update_input_layer(review.lower())\n",
        "\n",
        "        # Hidden layer\n",
        "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
        "\n",
        "        # Output layer\n",
        "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
        "        \n",
        "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
        "        # return NEGATIVE for other values\n",
        "        if(layer_2[0] >= 0.5):\n",
        "            return \"POSITIVE\"\n",
        "        else:\n",
        "            return \"NEGATIVE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVHy6qDns8Rs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 7: Keras\n",
        "\n",
        "Examples of packages for deep learning:\n",
        "\n",
        "* Keras\n",
        "* TensorFlow\n",
        "* Caffe\n",
        "* Theano\n",
        "* Scikit-learn\n",
        "* ..."
      ]
    },
    {
      "metadata": {
        "id": "h1OVnjb8tnTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a Sequential Model in Keras"
      ]
    },
    {
      "metadata": {
        "id": "W_IldXVBtmR0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "e7aaab4e-78b1-40a0-b6b6-27b2ec87ee57",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527792722789,
          "user_tz": -120,
          "elapsed": 7867,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "\n",
        "# Layers\n",
        "## Fully connected layers\n",
        "## max pool layers\n",
        "## activation layers\n",
        "## and more\n",
        "\n",
        "# X has shape (num_rows, num_cols) where the training data is stored as row vectors\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "\n",
        "# y must have an output vector for each input vector\n",
        "y = np.array([[0], [0], [0], [1]], dtype=np.float32)\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "# The keras.models.Sequential class is a wrapper for the neural network model\n",
        "# that treats the network as a sequence of layers\n",
        "\n",
        "# 1st layer - Add an input layer of 32 nodes with the same input shape as X\n",
        "model.add(Dense(32, input_dim=X.shape[1]))\n",
        "\n",
        "# Add a softmax activation layer\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# 2nd layer - Add a fully connected output layer\n",
        "model.add(Dense(1))\n",
        "\n",
        "#Add a sigmoid activation layer\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "X"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "zXP72mPLu-jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Keras requires the input shape to be specified for the first layer. It will automatically infer the shape of all other layers\n",
        "* Activation is added after the layers\n",
        "* Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function and other paramters required before the model can be run on any input data.\n",
        "* We specify the loss function to be *categorical_crossentropy*\n",
        "* and specify *adam* as the optimizer\n",
        "* also we specify what metrics we want to evaluate the model with"
      ]
    },
    {
      "metadata": {
        "id": "TaKhaBzgv0zl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
        "\n",
        "# See the resulting model architecture with the following command\n",
        "model.summary()\n",
        "\n",
        "# Train with the fit() method\n",
        "\n",
        "model.fit(X, y, epochs=1000, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "model.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bExrx9X3DcK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keras Optimizers\n",
        "\n",
        "**SGD - Stochastic Gradient Descent:** It uses the following paramters:\n",
        "* Learning rate.\n",
        "* Momentum\n",
        "* Nesterov Momentum (This slows down the gradient when it's close to the solution)\n",
        "\n",
        "**Adam:** Adaptive Moment Estimation uses a more complicated exponential decay that consists of not just considering the average (first moment), but also the variance (second moment) of the previous steps.\n",
        "\n",
        "**RMSProp:** RMSProp (RMS stands for Roo Mean Square Error) decreases the learning rate by dividing it by an exponentially decaying average of squared gradients.\n",
        "\n",
        "[Further explanation of Keras Optimizers](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)\n",
        "\n",
        "[Keras Documentation about Optimizers](https://keras.io/optimizers/)"
      ]
    },
    {
      "metadata": {
        "id": "JPauE_rn46um",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### IMDB Data in Keras"
      ]
    },
    {
      "metadata": {
        "id": "xcEO0dUS4W90",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "24ff5bb6-4f2e-497f-d8cf-52f9c46cdf10",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311507627,
          "user_tz": -120,
          "elapsed": 15741,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Loading the data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1220nLwD5m3i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0df3a161-5633-4323-d0fa-33ad7b4a5f82",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311508346,
          "user_tz": -120,
          "elapsed": 638,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(x_test[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ypg-wpBr51Jf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "10d5ca36-2c7d-4531-c164-e9a607b3446f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311515808,
          "user_tz": -120,
          "elapsed": 4758,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# One-hot encoding the output into vector mode, each of length 1000\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print(x_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            " 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KtO8E48j6Ddu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9fe3df0f-a941-4310-9dcb-1e2a81d8f7bf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311517689,
          "user_tz": -120,
          "elapsed": 608,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# One-hot encoding the output\n",
        "num_classes = 2\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 2)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jHnx8xKl6Pam",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "a94fe177-f161-41ea-e001-8e1c1be8f6ab",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311531197,
          "user_tz": -120,
          "elapsed": 658,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Build the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(8, input_dim=x_train.shape[1]))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2))\n",
        "# TODO: Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics = [\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 8)                 8008      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 18        \n",
            "=================================================================\n",
            "Total params: 8,026\n",
            "Trainable params: 8,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uVX7djx7m3m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "543954ac-2628-406f-fa2c-fe570b71b509",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528311585776,
          "user_tz": -120,
          "elapsed": 47804,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Training and evaluating\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test,y_test), verbose=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.7260 - acc: 0.4333 - val_loss: 0.6570 - val_acc: 0.3014\n",
            "Epoch 2/10\n",
            " - 5s - loss: 0.6421 - acc: 0.3476 - val_loss: 0.6156 - val_acc: 0.3400\n",
            "Epoch 3/10\n",
            " - 5s - loss: 0.8323 - acc: 0.4374 - val_loss: 0.8058 - val_acc: 0.5000\n",
            "Epoch 4/10\n",
            " - 5s - loss: 0.7665 - acc: 0.5000 - val_loss: 0.7368 - val_acc: 0.5000\n",
            "Epoch 5/10\n",
            " - 5s - loss: 0.7263 - acc: 0.5000 - val_loss: 0.7138 - val_acc: 0.5000\n",
            "Epoch 6/10\n",
            " - 5s - loss: 0.7101 - acc: 0.5000 - val_loss: 0.7035 - val_acc: 0.5000\n",
            "Epoch 7/10\n",
            " - 5s - loss: 0.7024 - acc: 0.5000 - val_loss: 0.6984 - val_acc: 0.5000\n",
            "Epoch 8/10\n",
            " - 5s - loss: 0.6985 - acc: 0.5000 - val_loss: 0.6956 - val_acc: 0.5000\n",
            "Epoch 9/10\n",
            " - 5s - loss: 0.6965 - acc: 0.4987 - val_loss: 0.6940 - val_acc: 0.5000\n",
            "Epoch 10/10\n",
            " - 5s - loss: 0.6955 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5bc9970e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "MSlywDozFBgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 8: TensorFlow\n",
        "\n",
        "Keras is good for building neural networks quickly, but it abstracts a lot of detail.\n",
        "TensorFlow is great for understanding how neural networks operate on a lower level."
      ]
    },
    {
      "metadata": {
        "id": "NgJdrYcTFnKf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50833969-81c2-4109-c58f-a1b43663943d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526848883386,
          "user_tz": -120,
          "elapsed": 606,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "hello_constant = tf.constant('Hello World!')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  output = sess.run(hello_constant)\n",
        "  print(output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello World!'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Swqg76-wFyke",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# A is a 0-dimensional int32 tensor\n",
        "A = tf.constant(1234)\n",
        "# B is a 1-dimensional int32 tensor\n",
        "B = tf.constant([123,456,789])\n",
        "# C is a 2-dimensional int32 tensor\n",
        "C = tf.constant([[123,456,789], [222,333,444]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT8_xUhEGMtK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Session\n",
        "\n",
        "The Tensorflow Session is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines\n",
        "\n",
        "#### tf.placeholder()\n",
        "\n",
        "tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs."
      ]
    },
    {
      "metadata": {
        "id": "xo9V9a08Go3K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cd9d68d-1c10-4835-ee03-a2e22b565d9d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526405729495,
          "user_tz": -120,
          "elapsed": 743,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.string)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  output = sess.run(x, feed_dict={x: \"Hello World\"})\n",
        "  print(output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dztv-r5gIjTX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4c86e80e-ce39-478c-d96c-6a7d083cdc63",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526405734157,
          "user_tz": -120,
          "elapsed": 610,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.string)\n",
        "y = tf.placeholder(tf.int32)\n",
        "z = tf.placeholder(tf.float32)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
        "    print(output)\n",
        "    print(y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test String\n",
            "Tensor(\"Placeholder_2:0\", dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bezYOxILIybQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d21b605-275d-4425-ee99-98566bde488b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526326606267,
          "user_tz": -120,
          "elapsed": 591,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Addition\n",
        "x = tf.add(5, 2)\n",
        "# Subtraction\n",
        "y = tf.subtract(10, 4)\n",
        "# Multiplication\n",
        "z = tf.multiply(2, 5)\n",
        "\n",
        "# Casting\n",
        "tf.cast(tf.constant(2.0), tf.int32)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Cast:0' shape=() dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "hHlNS0kgK_zR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear functions in TensorFlow\n",
        "\n",
        "The goal of training a NN is to modify weights and biases to best predict labls. Therfore we use tf.Variable\n",
        "\n",
        "tf.Variable is a tensor with an initial value that can be modified, much like a normal Python variable.This tensor stores its state in the session, so you must initialize the state of the tensor manually with tf.gloabl_variables_initializer()"
      ]
    },
    {
      "metadata": {
        "id": "Lgktg7DlSNzd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "x = tf.Variable(5)\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5AAF6oFSZuL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initializing the weights with random numbers from a normal distribution is good practice\n",
        "# -> tf.truncated_normal()\n",
        "\n",
        "n_features = 120\n",
        "n_labels = 5\n",
        "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPATQj3FSy44",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# tf.zeros()\n",
        "\n",
        "bias = tf.Variable(tf.zeros(n_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvqqN_6BS6l9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "a0c72d6f-d95c-47b7-bc67-f4167ba5c0c8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526848914224,
          "user_tz": -120,
          "elapsed": 7356,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_weights(n_features, n_labels):\n",
        "    \"\"\"\n",
        "    Return TensorFlow weights\n",
        "    :param n_features: Number of features\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow weights\n",
        "    \"\"\"\n",
        "    # TODO: Return weights\n",
        "    weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
        "    \n",
        "    return weights\n",
        "\n",
        "\n",
        "def get_biases(n_labels):\n",
        "    \"\"\"\n",
        "    Return TensorFlow bias\n",
        "    :param n_labels: Number of labels\n",
        "    :return: TensorFlow bias\n",
        "    \"\"\"\n",
        "    # TODO: Return biases\n",
        "    bias = tf.Variable(tf.zeros(n_labels))\n",
        "    \n",
        "    return bias\n",
        "\n",
        "\n",
        "def linear(input, w, b):\n",
        "    \"\"\"\n",
        "    Return linear function in TensorFlow\n",
        "    :param input: TensorFlow input\n",
        "    :param w: TensorFlow weights\n",
        "    :param b: TensorFlow biases\n",
        "    :return: TensorFlow linear function\n",
        "    \"\"\"\n",
        "    # TODO: Linear Function (xW + b)\n",
        "    return tf.add(tf.matmul(input,w),b)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "def mnist_features_labels(n_labels):\n",
        "    \"\"\"\n",
        "    Gets the first <n> labels from the MNIST dataset\n",
        "    :param n_labels: Number of labels to use\n",
        "    :return: Tuple of feature list and label list\n",
        "    \"\"\"\n",
        "    mnist_features = []\n",
        "    mnist_labels = []\n",
        "\n",
        "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
        "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
        "\n",
        "        # Add features and labels if it's for the first <n>th labels\n",
        "        if mnist_label[:n_labels].any():\n",
        "            mnist_features.append(mnist_feature)\n",
        "            mnist_labels.append(mnist_label[:n_labels])\n",
        "\n",
        "    return mnist_features, mnist_labels\n",
        "\n",
        "\n",
        "# Number of features (28*28 image is 784 features)\n",
        "n_features = 784\n",
        "# Number of labels\n",
        "n_labels = 3\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32)\n",
        "labels = tf.placeholder(tf.float32)\n",
        "\n",
        "# Weights and Biases\n",
        "w = get_weights(n_features, n_labels)\n",
        "b = get_biases(n_labels)\n",
        "\n",
        "# Linear Function xW + b\n",
        "logits = linear(features, w, b)\n",
        "\n",
        "# Training data\n",
        "train_features, train_labels = mnist_features_labels(n_labels)\n",
        "\n",
        "with tf.Session() as session:\n",
        "    # TODO: Initialize session variables\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # Softmax\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "\n",
        "    # Cross entropy\n",
        "    # This quantifies how far off the predictions were.\n",
        "    # You'll learn more about this in future lessons.\n",
        "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
        "\n",
        "    # Training loss\n",
        "    # You'll learn more about this in future lessons.\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "    # Rate at which the weights are changed\n",
        "    # You'll learn more about this in future lessons.\n",
        "    learning_rate = 0.08\n",
        "\n",
        "    # Gradient Descent\n",
        "    # This is the method used to train the model\n",
        "    # You'll learn more about this in future lessons.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "    # Run optimizer and get loss\n",
        "    _, l = session.run(\n",
        "        [optimizer, loss],\n",
        "        feed_dict={features: train_features, labels: train_labels})\n",
        "\n",
        "# Print loss\n",
        "print('Loss: {}'.format(l))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-30db138e172f>:53: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Loss: 3.5648272037506104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2PBzB5vBVWDA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Softmax\n",
        "\n",
        "The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1."
      ]
    },
    {
      "metadata": {
        "id": "3r5EasiIV6Al",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53d7ab08-1bfc-49a4-a2fc-2edfd46f0645",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526405809503,
          "user_tz": -120,
          "elapsed": 614,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def run():\n",
        "    output = None\n",
        "    logit_data = [2.0, 1.0, 0.1]\n",
        "    logits = tf.placeholder(tf.float32)\n",
        "    \n",
        "    # TODO: Calculate the softmax of the logits\n",
        "    softmax = tf.nn.softmax(logits)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        # TODO: Feed in the logit data\n",
        "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
        "\n",
        "    return output\n",
        "  \n",
        "run()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6590012 , 0.24243298, 0.09856589], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "hDkPKKplWCnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy in TensorFlow\n",
        "\n",
        "tf.reduce_sum() function takes an array of numbers and sums them together\n",
        "tf.log() is the natural log"
      ]
    },
    {
      "metadata": {
        "id": "o9WpsCtTWBHE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06938b9c-ea40-43dd-ed65-4aba3dd7b76c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526405849895,
          "user_tz": -120,
          "elapsed": 652,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Solution is available in the other \"solution.py\" tab\n",
        "import tensorflow as tf\n",
        "\n",
        "softmax_data = [0.7, 0.2, 0.1]\n",
        "one_hot_data = [1.0, 0.0, 0.0]\n",
        "\n",
        "softmax = tf.placeholder(tf.float32)\n",
        "one_hot = tf.placeholder(tf.float32)\n",
        "\n",
        "# TODO: Print cross entropy from session\n",
        "cross_entropy = - tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
        "with tf.Session() as session:\n",
        "    result = session.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
        "    \n",
        "    print(result)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FBLIGO6xXe75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mini-batching\n",
        "\n",
        "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time.\n",
        "\n",
        "Mini-batching is computationally inefficient, sinc eyou can't calculate the loss simultaneously across all samples.\n",
        "\n",
        "Combined with SGD => Randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "G4bjagNveBzu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def batches(batch_size, features, labels):\n",
        "    \"\"\"\n",
        "    Create batches of features and labels\n",
        "    :param batch_size: The batch size\n",
        "    :param features: List of features\n",
        "    :param labels: List of labels\n",
        "    :return: Batches of (Features, Labels)\n",
        "    \"\"\"\n",
        "    assert len(features) == len(labels)\n",
        "    # TODO: Implement batching\n",
        "    output_batches = []\n",
        "    \n",
        "    sample_size = len(features)\n",
        "    for start_i in range(0, sample_size, batch_size):\n",
        "        end_i = start_i + batch_size\n",
        "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
        "        output_batches.append(batch)\n",
        "        \n",
        "    return output_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXUSDYav54u0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "05017ad7-9968-4725-ebd7-6d7cb0493b88",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527792976260,
          "user_tz": -120,
          "elapsed": 8714,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.001\n",
        "n_input = 784  # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
        "\n",
        "# The features are already scaled and the data is shuffled\n",
        "train_features = mnist.train.images\n",
        "test_features = mnist.test.images\n",
        "\n",
        "train_labels = mnist.train.labels.astype(np.float32)\n",
        "test_labels = mnist.test.labels.astype(np.float32)\n",
        "\n",
        "# Features and Labels\n",
        "features = tf.placeholder(tf.float32, [None, n_input])\n",
        "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Weights & bias\n",
        "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
        "bias = tf.Variable(tf.random_normal([n_classes]))\n",
        "\n",
        "# Logits - xW + b\n",
        "logits = tf.add(tf.matmul(features, weights), bias)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# TODO: Set batch size\n",
        "batch_size = 128\n",
        "assert batch_size is not None, 'You must set the batch size'\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoch_i in range(epochs):\n",
        "      # TODO: Train optimizer on all batches\n",
        "      for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
        "          sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
        "\n",
        "      # Calculate accuracy for test dataset\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={features: test_features, labels: test_labels})\n",
        "\n",
        "print('Test Accuracy: {}'.format(test_accuracy))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Test Accuracy: 0.2583000063896179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vN2TvpJK6Fuw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Epochs\n",
        "\n",
        "An epoch is a single forward and backward pass of the whole dataset.\n",
        "\n",
        "**Added epochs in code above**"
      ]
    },
    {
      "metadata": {
        "id": "0ow84lnl_9zg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e65e195-b825-4efd-d7a5-03c36af200df",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526408053952,
          "user_tz": -120,
          "elapsed": 827,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "\n",
        "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
        "def normalize_grayscale(image_data):\n",
        "    \"\"\"\n",
        "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
        "    :param image_data: The image data to be normalized\n",
        "    :return: Normalized image data\n",
        "    \"\"\"\n",
        "    # TODO: Implement Min-Max scaling for grayscale image data\n",
        "    a = 0.1\n",
        "    b = 0.9\n",
        "    \n",
        "    minimum = image_data.min()\n",
        "    maximum = image_data.max()\n",
        "    \n",
        "    result = np.array([])\n",
        "        \n",
        "    for i in range(len(image_data)):\n",
        "        result = np.append(result, (a - (((image_data[i] - minimum) * (a - b)) / (maximum - minimum))))\n",
        "        \n",
        "    return result\n",
        "\n",
        "### DON'T MODIFY ANYTHING BELOW ###\n",
        "# Test Cases\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
        "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
        "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
        "    decimal=3)\n",
        "np.testing.assert_array_almost_equal(\n",
        "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
        "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
        "     0.896862745098, 0.9])\n",
        "\n",
        "print('Tests Passed!')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "te0wZwOWELYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deeper Neural Networks with Tensorflow\n",
        "\n",
        "Non-linear function in the hidden layer to solve non-linear problems.\n",
        "\n",
        "ReLU: tf.nn.relu()"
      ]
    },
    {
      "metadata": {
        "id": "5NonbWl2GI1k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "91aa48ba-e3c1-4e68-cc4f-d29c3914256b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526409574298,
          "user_tz": -120,
          "elapsed": 756,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Solution is available in the other \"solution.py\" tab\n",
        "import tensorflow as tf\n",
        "\n",
        "hidden_layer_weights = [\n",
        "    [0.1, 0.2, 0.4],\n",
        "    [0.4, 0.6, 0.6],\n",
        "    [0.5, 0.9, 0.1],\n",
        "    [0.8, 0.2, 0.8]]\n",
        "out_weights = [\n",
        "    [0.1, 0.6],\n",
        "    [0.2, 0.1],\n",
        "    [0.7, 0.9]]\n",
        "\n",
        "# Weights and biases\n",
        "weights = [\n",
        "    tf.Variable(hidden_layer_weights),\n",
        "    tf.Variable(out_weights)]\n",
        "biases = [\n",
        "    tf.Variable(tf.zeros(3)),\n",
        "    tf.Variable(tf.zeros(2))]\n",
        "\n",
        "# Input\n",
        "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
        "\n",
        "# TODO: Create Model\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "\n",
        "output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(sess.run(output))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5.11      8.440001]\n",
            " [ 0.        0.      ]\n",
            " [24.01     38.24    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5Xj-g51GqzY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deep Neural Network in TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "ifL6C5VOGbOb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e37e18a8-1eb9-4327-ff61-59c5c5dfda42",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526409674918,
          "user_tz": -120,
          "elapsed": 1939,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KKijVxAmGzgj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20\n",
        "batch_size = 128\n",
        "display_step = 1\n",
        "\n",
        "n_input = 784\n",
        "n_classes = 10\n",
        "\n",
        "n_hidden_layer = 256\n",
        "\n",
        "weights = {\n",
        "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lqTdeF6pHevt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "x_flat = tf.reshape(x, [-1, n_input])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83vQl_KWIIoo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Hidden layer with RELU activation\n",
        "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
        "layer_1 = tf.nn.relu(layer_1)\n",
        "# Output layer with linear activation\n",
        "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oseu8SL_IOAS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yt4wWLsRIV3_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5E-J3BLkIn1v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save and Restor TensorFlow Models"
      ]
    },
    {
      "metadata": {
        "id": "EuT9DRUvIwnJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b25dcd0e-5ae3-40c3-a10e-5afd11bb3a95",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526410216053,
          "user_tz": -120,
          "elapsed": 1407,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# The file path to save the data\n",
        "save_file = './model.ckpt'\n",
        "\n",
        "# Two Tensor Variables: weights and bias\n",
        "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "\n",
        "# Class used to save and/or restore Tensor Variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize all the Variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Show the values of weights and bias\n",
        "    print('Weights:')\n",
        "    print(sess.run(weights))\n",
        "    print('Bias:')\n",
        "    print(sess.run(bias))\n",
        "\n",
        "    # Save the model\n",
        "    saver.save(sess, save_file)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            "[[-0.22731961  0.81917626  0.21826631]\n",
            " [ 0.9458838   1.2711861  -0.43904358]]\n",
            "Bias:\n",
            "[-1.230412  -1.6085608 -1.645819 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fIUtwGtLI3wn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Remove the previous weights and bias\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Two Variables: weights and bias\n",
        "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
        "bias = tf.Variable(tf.truncated_normal([3]))\n",
        "\n",
        "# Class used to save and/or restore Tensor Variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load the weights and bias\n",
        "    saver.restore(sess, save_file)\n",
        "\n",
        "    # Show the values of weights and bias\n",
        "    print('Weight:')\n",
        "    print(sess.run(weights))\n",
        "    print('Bias:')\n",
        "    print(sess.run(bias))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVZQEnZuKeDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Dropout"
      ]
    },
    {
      "metadata": {
        "id": "_7NSgU7HJBqh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "95c66069-e723-4bf8-e8ac-4cca0d1537c1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526410987633,
          "user_tz": -120,
          "elapsed": 760,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Solution is available in the other \"solution.py\" tab\n",
        "import tensorflow as tf\n",
        "\n",
        "hidden_layer_weights = [\n",
        "    [0.1, 0.2, 0.4],\n",
        "    [0.4, 0.6, 0.6],\n",
        "    [0.5, 0.9, 0.1],\n",
        "    [0.8, 0.2, 0.8]]\n",
        "out_weights = [\n",
        "    [0.1, 0.6],\n",
        "    [0.2, 0.1],\n",
        "    [0.7, 0.9]]\n",
        "\n",
        "# Weights and biases\n",
        "weights = [\n",
        "    tf.Variable(hidden_layer_weights),\n",
        "    tf.Variable(out_weights)]\n",
        "biases = [\n",
        "    tf.Variable(tf.zeros(3)),\n",
        "    tf.Variable(tf.zeros(2))]\n",
        "\n",
        "# Input\n",
        "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
        "\n",
        "# TODO: Create Model with Dropout\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
        "\n",
        "output_layer = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  output = sess.run(output_layer, feed_dict={keep_prob: 0.5})\n",
        "  print(output)\n",
        "\n",
        "# TODO: Print logits from a session\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.8800001   0.94000006]\n",
            " [ 0.112       0.67200005]\n",
            " [33.74       43.38      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HUlzN-soNHyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "## Lesson 2: Convolutional Neural Networks\n",
        "\n",
        "RNN are used more than CNNs\n",
        "\n",
        "**categorical_crossentropy as loss function:** The loss will be\n",
        "* Lower: When label and prediction agree\n",
        "* Higher: When label and prediction disagree\n",
        "\n",
        "**MLP:**\n",
        "* Only use fully connected layers\n",
        "* Only accept vectors\n",
        "\n",
        "**CNNs:**\n",
        "* Sparsely connected layers\n",
        "* Accept matrices as input\n",
        "\n",
        "Multiply each node with the weights, add i all up and add the bias (and activation function) => **Filter.**\n",
        "\n",
        "For color images the FIlter is 3D. (RGB)\n",
        "\n",
        "**Stride:** How are to move around the image.\n",
        "\n",
        "**Padding:** What to do at the edges of the image? Padding the Image with zeros.\n",
        "\n",
        "*   Valid: Loose information on the edges\n",
        "*   Same: Padding is applied\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oB6Lgh71Tu-6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D\n",
        "\n",
        "# Create a convolutional layer:\n",
        "filters = 1\n",
        "kernel_size = 2\n",
        "\n",
        "Conv2D(filters, kernel_size, strides, padding='same', activiation='relu', input_shape)\n",
        "\n",
        "## mandatory fields\n",
        "# filters: number of filters\n",
        "# kernel_size: Number specifying both the height and the width of the (square) convolution window\n",
        "# input_shape: If the first layer, it must specify the height, width and depth of the input\n",
        "\n",
        "## optional fields\n",
        "# strides: strides of the convolution\n",
        "# padding: valid or same\n",
        "# activation: typically relu\n",
        "\n",
        "Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ke3Vo93yUPNp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "45019435-2394-4299-b0f9-20f3c1df0e31",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526849100735,
          "user_tz": -120,
          "elapsed": 957,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Dimensionality\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='valid', activation='relu', input_shape=(200, 200, 1)))\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 100, 100, 16)      80        \n",
            "=================================================================\n",
            "Total params: 80\n",
            "Trainable params: 80\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EGmLnVAuV0aU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Number of Parameters in a CNN\n",
        "\n",
        "* **K** - # of filters\n",
        "* **F** - height and width of the CNN filters\n",
        "* **D_in** - the depth of the previous layer\n",
        "\n",
        "K*F*F*D_in + K\n",
        "\n",
        "### Shape of a Convolutional Layer\n",
        "\n",
        "* **S** - stride of the convolution\n",
        "* **H_in** - the height of the previous layer\n",
        "* **W_in** - the width of the previous layer\n",
        "\n",
        "If padding is **same**:\n",
        "\n",
        "* height: ceil(float(H_in) / float(S))\n",
        "* width: ceil(float(W_in) / float(S))\n",
        "\n",
        "if padding is **valid**:\n",
        "\n",
        "* height: ceil(float(H_in - F + 1 / float(S)))\n",
        "* widt: ceil(float(W_in - F + 1 / float(S)))"
      ]
    },
    {
      "metadata": {
        "id": "n9sYAgGmVNrZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "fff79351-dc80-4ca9-8f32-c1a30a73a9aa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526497787118,
          "user_tz": -120,
          "elapsed": 611,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding='same', \n",
        "    activation='relu', input_shape=(128, 128, 3)))\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 32)        896       \n",
            "=================================================================\n",
            "Total params: 896\n",
            "Trainable params: 896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RSEagT_xXVtA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Type of layer for CNNs\n",
        "\n",
        "Dimensionality of the convolutional layer can lead to overfitting => reduce dimensionality. Reduce width and height.\n",
        "\n",
        "* max pooling layer: Take a stack of features maps as input. Run a convolution across the convolutional layer. \n",
        "* gloval average pooling layer: computes the average value of eath feature map in a convolution => More drastic reduction"
      ]
    },
    {
      "metadata": {
        "id": "OUTjpBh1W7nj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "MaxPooling2D(pool_size, strides,padding)\n",
        "\n",
        "## Mandatory arguments\n",
        "# pool_size - Number specifiying the height and width of the pooling window\n",
        "## Optional arguments\n",
        "# strides - vertical and horizontal stride\n",
        "# padding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPyt4_9_cGuf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cc049bdc-ddae-43d0-a5a1-f09223018bef",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526849139654,
          "user_tz": -120,
          "elapsed": 765,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 15)        0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wxylSApGcWCY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "faba90a6-e9d3-4ae7-d8d9-60830cc92955",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526849149491,
          "user_tz": -120,
          "elapsed": 809,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 16)        208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 32)          4128      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               256500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 267,926\n",
            "Trainable params: 267,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GVUTLKQ5mrjI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation in Keras\n",
        "\n",
        "We want to learn an invariant image:\n",
        "* Scale Invariance\n",
        "* Rotation Invariance\n",
        "* Translation Invariance\n",
        "\n",
        "Add images to the training set by doing some rotation, translation on the training set."
      ]
    },
    {
      "metadata": {
        "id": "E574zFVspRhW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# create and configure augmented image generator\n",
        "datagen_train = ImageDataGenerator(\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n",
        "    horizontal_flip=True) # randomly flip images horizontally\n",
        "\n",
        "# fit augmented image generator on data\n",
        "datagen_train.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PauQp1ADo32b",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    epochs=epochs, verbose=2, callbacks=[checkpointer],\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    validation_steps=x_valid.shape[0] // batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-lrdKjHptLM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# steps_per_epoch = x_train.shape[0] / batch_size\n",
        "where x_train.shape[0] corresponds to the number of unique samples in the training dataset x_train.\n",
        "By setting steps_per_epoch to this value, we ensure that the model sees x_train.shape[0] augmented images in each epoch."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p0NoEuiZpsPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**AlexNet:** Pioneered the use of the ReLU activation function as well as dropout\n",
        "\n",
        "**VGG:** VGG16 and VGG 19 with the total layers as the numbers\n",
        "\n",
        "**ResNet:** Like VGG. 3 convolutional layer.\n",
        "\n",
        "Too many layers => Vanishing gradient problem."
      ]
    },
    {
      "metadata": {
        "id": "D0Tym9cwrwaF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing CNNs\n",
        "\n",
        "Deep Dreams: Using a input picture with a filter that recognizes something else."
      ]
    },
    {
      "metadata": {
        "id": "udysXmVguI-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning\n",
        "\n",
        "Using a CNN from a training set with new images.\n",
        "\n",
        "The filters later in the architecture are more specific to the training network. It depends on both on size of the new data and the similiarity of the new data set to the original data set.\n",
        "\n",
        "**The four main cases:**\n",
        "1. new data set is small, new data is similiar to original training data\n",
        "2. new data set is small, new data is different from original training data\n",
        "3. new data set is large, new data is simimiliar to original training data\n",
        "4. new data set is large, new data is different from original training data\n",
        "\n",
        "#### Demonstration Network\n",
        "\n",
        "What the CNN does:\n",
        "\n",
        "1. first layer detects edges of the image\n",
        "2. the second layer will detect shapes\n",
        "3. the third convolutional layer detects higher level features\n",
        "\n",
        "##### Case 1: Small Data set, Similiar Data\n",
        "\n",
        "* slice off the end of the neural network\n",
        "* add a new fully connected layer that matches the number of classes in the new data set\n",
        "* randomize the weights of the new fully connected layer; freeze all the weights from the pre-trained network\n",
        "* train the network to update the weights of the new fully connected layer\n",
        "\n",
        "##### Case 2: Small Data set, Similiar Data\n",
        "\n",
        "* slice off most of the pre-trained layers near the beginning of the network\n",
        "* add to the reminaing pre-trained layers a new fully connected layer that matches the number of classes in the new dataset\n",
        "* randomize the weights of the new fully connected layer; freeze all the weights from the pre-trained network\n",
        "* train the network to update the weights of the new fully connected layer\n",
        "\n",
        "##### Case 3: Large Data set, Similiar Data\n",
        "\n",
        "* remove the last fully connected layer and replace with a layer matching the number of classes in the new data set\n",
        "* randomly initialize the weights in the new fully connected layer\n",
        "* initialize the rest of the weights using the pre-trained weights\n",
        "* re-train the entire neural network\n",
        "\n",
        "##### Case 4: Large Data set, Different Data\n",
        "\n",
        "* remove the last fully connected layer and replace with a layer matching the number of classes int he new data set\n",
        "* retrain the network from scratch with randomly initialized weights\n",
        "* alternatively, you could use the same strategy as the \"large and similiar\" data case\n"
      ]
    },
    {
      "metadata": {
        "id": "hTt2aDCYypIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning in Keras\n",
        "\n",
        "Using VGG-16 for the dog breeder classification."
      ]
    },
    {
      "metadata": {
        "id": "-zIBWRydThBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 3: CNNs in Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "b_FK-FIgTlZM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# TensorFlow provices:\n",
        "## tf.nn.conv2d(), tf.nn.bias_add() and tf.nn.relu() to create convolutional layers\n",
        "\n",
        "# output depth\n",
        "k_output = 64\n",
        "\n",
        "# image dimensions\n",
        "image_width = 10\n",
        "image_height = 10\n",
        "color_channels = 3\n",
        "\n",
        "# convolution filter dimensions\n",
        "filter_size_width = 5\n",
        "filter_size_height = 5\n",
        "\n",
        "# input/image\n",
        "input = tf.placeholder(\n",
        "  tf.float32,\n",
        "  shape=[None, image_height, image_width, color_channels])\n",
        "\n",
        "# weight and bias\n",
        "weight = tf.Variable(tf.truncated_normal(\n",
        "  [filter_size_height, filter_size_width, color_channels, k_output]))\n",
        "bias = tf.Variable(tf.zeros(k_output))\n",
        "\n",
        "# apply convolution\n",
        "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
        "## TensorFlow uses a stride vor each input dimension [batch, input_height, input_width, input_channels]\n",
        "\n",
        "# add bias\n",
        "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
        "# apply activation function\n",
        "conv_layer = tf.nn.relu(conv_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJeOOixBcci6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Setup the strides, padding and filter weight/bias such that\n",
        "the output shape is (1, 2, 2, 3).\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
        "# (1, 4, 4, 1)\n",
        "x = np.array([\n",
        "    [0, 1, 0.5, 10],\n",
        "    [2, 2.5, 1, -8],\n",
        "    [4, 0, 5, 6],\n",
        "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
        "X = tf.constant(x)\n",
        "\n",
        "\n",
        "def conv2d(input):\n",
        "    # Filter (weights and bias)\n",
        "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
        "    # The shape of the filter bias is (output_depth,)\n",
        "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
        "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
        "    F_W = tf.Variable(tf.truncated_normal(\n",
        "        [1, 1, 1, 3]))\n",
        "    F_b = tf.Variable(tf.zeros(3))\n",
        "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
        "    strides = [1, 2, 2, 1]\n",
        "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
        "    padding = 'SAME'\n",
        "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
        "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
        "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
        "\n",
        "out = conv2d(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dkob6_sxfDvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Calculation of new height/width\n",
        "\n",
        "**SAME** Padding, the output height and width are computed as:\n",
        "\n",
        "* out_height = ceil(float(in_height) / float(strides[1]))\n",
        "* out_width = ceil(float(in_width) / float(strides[2]))\n",
        "\n",
        "**VALID** Padding, the output height and width are computed as:\n",
        "\n",
        "* out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
        "* out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))"
      ]
    },
    {
      "metadata": {
        "id": "tg-9Eh49f_xn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Max Pooling Layers in TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "a1AAQkuxgCK9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "conv_layer = tf.cnn.conv2d(input, weight, strides=[1,2,2,1], padding='SAME')\n",
        "conv_layer = tf.bias_add(conv_layer, bias)\n",
        "conv_layer = tf.nn.relu(conv_layer)\n",
        "\n",
        "# apply max pooling\n",
        "conv_layer = tf.nn.max_pool(\n",
        "  conv_layer,\n",
        "  ksize=[1,2,2,1],\n",
        "  strides=[1,2,2,1],\n",
        "  padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckMbDJ9ghCpc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNNs in TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "zfPhItlEggS4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "e1cb83db-edf5-4b77-f9a5-9baf1996b179",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528312005419,
          "user_tz": -120,
          "elapsed": 8666,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.00001\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "\n",
        "# number of samples to calculate validation and accuracy\n",
        "# Decrease this if you are running out of memory\n",
        "\n",
        "test_valid_size = 256\n",
        "\n",
        "# network paramters\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.75 # dropout (probability to keep units)\n",
        "\n",
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "  \n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(\n",
        "        x,\n",
        "        ksize=[1, k, k, 1],\n",
        "        strides=[1, k, k, 1],\n",
        "        padding='SAME')\n",
        "  \n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # Layer 1 - 28*28*1 to 14*14*32\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Layer 2 - 14*14*32 to 7*7*64\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer - 7*7*64 to 1024\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output Layer - class prediction - 1024 to 10\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-9fc4db133a21>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IL6il63_iXa4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1076
        },
        "outputId": "83a2705f-e8a9-4958-ae61-d8120b28e6d1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528313049314,
          "user_tz": -120,
          "elapsed": 815710,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# Model\n",
        "logits = conv_net(x, weights, biases, keep_prob)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(\\\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
        "    .minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf. global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(mnist.train.num_examples//batch_size):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            sess.run(optimizer, feed_dict={\n",
        "                x: batch_x,\n",
        "                y: batch_y,\n",
        "                keep_prob: dropout})\n",
        "\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss = sess.run(cost, feed_dict={\n",
        "                x: batch_x,\n",
        "                y: batch_y,\n",
        "                keep_prob: 1.})\n",
        "            valid_acc = sess.run(accuracy, feed_dict={\n",
        "                x: mnist.validation.images[:test_valid_size],\n",
        "                y: mnist.validation.labels[:test_valid_size],\n",
        "                keep_prob: 1.})\n",
        "            \n",
        "            if (batch % 1000 == 0):\n",
        "              print('Epoch {:>2}, Batch {:>3} -'\n",
        "                    'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
        "                  epoch + 1,\n",
        "                  batch + 1,\n",
        "                  loss,\n",
        "                  valid_acc))\n",
        "\n",
        "    # Calculate Test Accuracy\n",
        "    test_acc = sess.run(accuracy, feed_dict={\n",
        "        x: mnist.test.images[:test_valid_size],\n",
        "        y: mnist.test.labels[:test_valid_size],\n",
        "        keep_prob: 1.})\n",
        "    print('Testing Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1, Batch   1 -Loss: 69151.8672 Validation Accuracy: 0.074219\n",
            "Epoch  1, Batch 1001 -Loss:   168.6952 Validation Accuracy: 0.753906\n",
            "Epoch  1, Batch 2001 -Loss:   468.3845 Validation Accuracy: 0.746094\n",
            "Epoch  1, Batch 3001 -Loss:   384.1364 Validation Accuracy: 0.789062\n",
            "Epoch  1, Batch 4001 -Loss:    28.5592 Validation Accuracy: 0.800781\n",
            "Epoch  1, Batch 5001 -Loss:     0.0000 Validation Accuracy: 0.820312\n",
            "Epoch  2, Batch   1 -Loss:     0.0000 Validation Accuracy: 0.820312\n",
            "Epoch  2, Batch 1001 -Loss:    32.6033 Validation Accuracy: 0.808594\n",
            "Epoch  2, Batch 2001 -Loss:     4.8289 Validation Accuracy: 0.812500\n",
            "Epoch  2, Batch 3001 -Loss:     0.0000 Validation Accuracy: 0.808594\n",
            "Epoch  2, Batch 4001 -Loss:   236.8294 Validation Accuracy: 0.824219\n",
            "Epoch  2, Batch 5001 -Loss:     0.0000 Validation Accuracy: 0.859375\n",
            "Epoch  3, Batch   1 -Loss:   122.0333 Validation Accuracy: 0.820312\n",
            "Epoch  3, Batch 1001 -Loss:     1.7448 Validation Accuracy: 0.824219\n",
            "Epoch  3, Batch 2001 -Loss:   208.0293 Validation Accuracy: 0.843750\n",
            "Epoch  3, Batch 3001 -Loss:    83.8877 Validation Accuracy: 0.843750\n",
            "Epoch  3, Batch 4001 -Loss:     2.5674 Validation Accuracy: 0.804688\n",
            "Epoch  3, Batch 5001 -Loss:    57.9413 Validation Accuracy: 0.832031\n",
            "Epoch  4, Batch   1 -Loss:    37.1266 Validation Accuracy: 0.847656\n",
            "Epoch  4, Batch 1001 -Loss:    14.1432 Validation Accuracy: 0.816406\n",
            "Epoch  4, Batch 2001 -Loss:    20.3813 Validation Accuracy: 0.789062\n",
            "Epoch  4, Batch 3001 -Loss:    19.0089 Validation Accuracy: 0.824219\n",
            "Epoch  4, Batch 4001 -Loss:     0.0000 Validation Accuracy: 0.820312\n",
            "Epoch  4, Batch 5001 -Loss:    23.3867 Validation Accuracy: 0.859375\n",
            "Epoch  5, Batch   1 -Loss:     1.6256 Validation Accuracy: 0.816406\n",
            "Epoch  5, Batch 1001 -Loss:     4.6709 Validation Accuracy: 0.824219\n",
            "Epoch  5, Batch 2001 -Loss:    27.0151 Validation Accuracy: 0.839844\n",
            "Epoch  5, Batch 3001 -Loss:    89.0259 Validation Accuracy: 0.835938\n",
            "Epoch  5, Batch 4001 -Loss:     9.0168 Validation Accuracy: 0.871094\n",
            "Epoch  5, Batch 5001 -Loss:     0.0000 Validation Accuracy: 0.851562\n",
            "Epoch  6, Batch   1 -Loss:    31.2648 Validation Accuracy: 0.847656\n",
            "Epoch  6, Batch 1001 -Loss:     5.1475 Validation Accuracy: 0.851562\n",
            "Epoch  6, Batch 2001 -Loss:    24.7841 Validation Accuracy: 0.855469\n",
            "Epoch  6, Batch 3001 -Loss:    29.9956 Validation Accuracy: 0.851562\n",
            "Epoch  6, Batch 4001 -Loss:    12.1864 Validation Accuracy: 0.835938\n",
            "Epoch  6, Batch 5001 -Loss:    13.3722 Validation Accuracy: 0.777344\n",
            "Epoch  7, Batch   1 -Loss:    10.0609 Validation Accuracy: 0.804688\n",
            "Epoch  7, Batch 1001 -Loss:    33.1072 Validation Accuracy: 0.843750\n",
            "Epoch  7, Batch 2001 -Loss:    18.1375 Validation Accuracy: 0.828125\n",
            "Epoch  7, Batch 3001 -Loss:    53.0244 Validation Accuracy: 0.792969\n",
            "Epoch  7, Batch 4001 -Loss:     6.4880 Validation Accuracy: 0.820312\n",
            "Epoch  7, Batch 5001 -Loss:    30.2499 Validation Accuracy: 0.812500\n",
            "Epoch  8, Batch   1 -Loss:    31.0123 Validation Accuracy: 0.757812\n",
            "Epoch  8, Batch 1001 -Loss:     0.0000 Validation Accuracy: 0.843750\n",
            "Epoch  8, Batch 2001 -Loss:    18.4031 Validation Accuracy: 0.843750\n",
            "Epoch  8, Batch 3001 -Loss:     1.1463 Validation Accuracy: 0.824219\n",
            "Epoch  8, Batch 4001 -Loss:    33.5510 Validation Accuracy: 0.843750\n",
            "Epoch  8, Batch 5001 -Loss:     5.4849 Validation Accuracy: 0.789062\n",
            "Epoch  9, Batch   1 -Loss:    13.6477 Validation Accuracy: 0.843750\n",
            "Epoch  9, Batch 1001 -Loss:     0.0000 Validation Accuracy: 0.816406\n",
            "Epoch  9, Batch 2001 -Loss:     8.3855 Validation Accuracy: 0.839844\n",
            "Epoch  9, Batch 3001 -Loss:    56.7136 Validation Accuracy: 0.812500\n",
            "Epoch  9, Batch 4001 -Loss:     0.0000 Validation Accuracy: 0.816406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  9, Batch 5001 -Loss:    37.2516 Validation Accuracy: 0.726562\n",
            "Epoch 10, Batch   1 -Loss:     5.4371 Validation Accuracy: 0.843750\n",
            "Epoch 10, Batch 1001 -Loss:     5.1812 Validation Accuracy: 0.820312\n",
            "Epoch 10, Batch 2001 -Loss:     6.4370 Validation Accuracy: 0.804688\n",
            "Epoch 10, Batch 3001 -Loss:     0.0000 Validation Accuracy: 0.820312\n",
            "Epoch 10, Batch 4001 -Loss:     0.0000 Validation Accuracy: 0.847656\n",
            "Epoch 10, Batch 5001 -Loss:    25.9393 Validation Accuracy: 0.832031\n",
            "Testing Accuracy: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X9W7Z71HjbD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 4: Weight Initialization\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iC4NmbAbm9ib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Uniform Distribution\n",
        "\n",
        "Equal probability of picking any number from a set of numbers. We'll be picking from a continous distribution, so the chance of picking the same number is low. We'll use TensorFlow's tf.random_uniform function to pick random numbers from a uniform distribution.\n",
        "\n",
        "> **tf.random_uniform**(shape, minival=0, maxval=None, dtype=tf.float32, seed=None, name=None)\n",
        "\n",
        "**General Rule for weight setting**:\n",
        "\n",
        "> The general rule for setting the weights in a neural network is to be close to zero without being too small. A good pracitce is to start your weights in the range of $[-y, y]$ where\n",
        "$y=1/\\sqrt{n}$ ($n$ is the number of inputs to a given neuron)."
      ]
    },
    {
      "metadata": {
        "id": "_cAkyxyHpgn3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normal Distribution\n",
        "\n",
        "Unlike the uniform distribution, the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) has a higher likelihood of picking number close to it's mean. To visualize it, let's plot values from TensorFlow's `tf.random_normal` function to a histogram.\n",
        "\n",
        "> >[tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)](https://www.tensorflow.org/api_docs/python/tf/random_normal)"
      ]
    },
    {
      "metadata": {
        "id": "SuBw-0irp4_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Truncated Normal Distribution\n",
        "\n",
        "The normal distribution gave a slight increasse in accuracy and loss.  Let's move closer to 0 and drop picked numbers that are `x` number of standard deviations away.  This distribution is called [Truncated Normal Distribution](https://en.wikipedia.org/wiki/Truncated_normal_distribution%29).\n",
        "\n",
        ">[tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)"
      ]
    },
    {
      "metadata": {
        "id": "ZxESwkbtq8bo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 5: Autoencoders\n",
        "\n",
        "Autoencoders are used to compress data, as well as image denoising.\n",
        "\n",
        "Autoencoders are bad at compressing data or generalizing to other datasets.\n",
        "But, they are good at image denoising and dimensionality redution."
      ]
    },
    {
      "metadata": {
        "id": "IJ5L0ZVwJBGt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "6ff94c11-48cd-40b2-814d-3cad44a209ea",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526907758070,
          "user_tz": -120,
          "elapsed": 8189,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-10ea3f74cab5>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8wdNnIAKVJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building basically the simplest autoencoder with a **single ReLU hidden layer**. This layer will be used as the compressed representation. Then, the encoder is the input layer and the hidden layer. The decoder is the hidden layer and the output layer. Since the images are normalized between 0 and 1, we need to use a **sigmoid activation on the output layer** to get values matching the input.\n",
        "\n",
        "> **Exercise:** Build the graph for the autoencoder in the cell below. The input images will be flattened into 784 length vectors. The targets are the same as the inputs. And there should be one hidden layer with a ReLU activation and an output layer with a sigmoid activation. Feel free to use TensorFlow's higher level API, `tf.layers`. For instance, you would use [`tf.layers.dense(inputs, units, activation=tf.nn.relu)`](https://www.tensorflow.org/api_docs/python/tf/layers/dense) to create a fully connected layer with a ReLU activation. The loss should be calculated with the cross-entropy loss, there is a convenient TensorFlow function for this `tf.nn.sigmoid_cross_entropy_with_logits` ([documentation](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)). You should note that `tf.nn.sigmoid_cross_entropy_with_logits` takes the logits, but to get the reconstructed images you'll need to pass the logits through the sigmoid function."
      ]
    },
    {
      "metadata": {
        "id": "OWoXkRc9JEeO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Size of the encoding layer (the hidden layer)\n",
        "encoding_dim = 32 # feel free to change this value\n",
        "\n",
        "image_size = mnist.train.images.shape[1]\n",
        "\n",
        "# Input and target placeholders\n",
        "inputs_ = tf.placeholder(tf.float32, [None, image_size])\n",
        "targets_ = tf.placeholder(tf.float32, [None, image_size])\n",
        "\n",
        "# Output of hidden layer, single fully connected layer here with ReLU activation\n",
        "encoded = tf.layers.dense(inputs_, encoding_dim, activation=tf.nn.relu)\n",
        "\n",
        "# Output layer logits, fully connected layer with no activation\n",
        "logits = tf.layers.dense(encoded, image_size, activation=None)\n",
        "# Sigmoid output from logits\n",
        "decoded = tf.sigmoid(logits) \n",
        "\n",
        "# Sigmoid cross-entropy loss\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
        "# Mean of the loss\n",
        "cost = tf.reduce_mean(loss)\n",
        "\n",
        "# Adam optimizer\n",
        "opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tM0xDcD5RhkU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create the session\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kx-WrtwzRkID",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "19746f5a-6dc0-4103-a38a-5edb713e097b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526899109876,
          "user_tz": -120,
          "elapsed": 67196,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 200\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for e in range(epochs):\n",
        "    for ii in range(mnist.train.num_examples//batch_size):\n",
        "        batch = mnist.train.next_batch(batch_size)\n",
        "        feed = {inputs_: batch[0], targets_: batch[0]}\n",
        "        batch_cost, _ = sess.run([cost, opt], feed_dict=feed)\n",
        "        \n",
        "        if(ii %200 == 0):\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Training loss: {:.4f}\".format(batch_cost))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Training loss: 0.6961\n",
            "Epoch: 1/20... Training loss: 0.2015\n",
            "Epoch: 2/20... Training loss: 0.1777\n",
            "Epoch: 2/20... Training loss: 0.1471\n",
            "Epoch: 3/20... Training loss: 0.1419\n",
            "Epoch: 3/20... Training loss: 0.1307\n",
            "Epoch: 4/20... Training loss: 0.1291\n",
            "Epoch: 4/20... Training loss: 0.1172\n",
            "Epoch: 5/20... Training loss: 0.1151\n",
            "Epoch: 5/20... Training loss: 0.1097\n",
            "Epoch: 6/20... Training loss: 0.1076\n",
            "Epoch: 6/20... Training loss: 0.1039\n",
            "Epoch: 7/20... Training loss: 0.1005\n",
            "Epoch: 7/20... Training loss: 0.0993\n",
            "Epoch: 8/20... Training loss: 0.0994\n",
            "Epoch: 8/20... Training loss: 0.1008\n",
            "Epoch: 9/20... Training loss: 0.0980\n",
            "Epoch: 9/20... Training loss: 0.0987\n",
            "Epoch: 10/20... Training loss: 0.0994\n",
            "Epoch: 10/20... Training loss: 0.0932\n",
            "Epoch: 11/20... Training loss: 0.0988\n",
            "Epoch: 11/20... Training loss: 0.0967\n",
            "Epoch: 12/20... Training loss: 0.0952\n",
            "Epoch: 12/20... Training loss: 0.0945\n",
            "Epoch: 13/20... Training loss: 0.0985\n",
            "Epoch: 13/20... Training loss: 0.0922\n",
            "Epoch: 14/20... Training loss: 0.0971\n",
            "Epoch: 14/20... Training loss: 0.0917\n",
            "Epoch: 15/20... Training loss: 0.0915\n",
            "Epoch: 15/20... Training loss: 0.0940\n",
            "Epoch: 16/20... Training loss: 0.0952\n",
            "Epoch: 16/20... Training loss: 0.0911\n",
            "Epoch: 17/20... Training loss: 0.0945\n",
            "Epoch: 17/20... Training loss: 0.0945\n",
            "Epoch: 18/20... Training loss: 0.0936\n",
            "Epoch: 18/20... Training loss: 0.0955\n",
            "Epoch: 19/20... Training loss: 0.0932\n",
            "Epoch: 19/20... Training loss: 0.0954\n",
            "Epoch: 20/20... Training loss: 0.0928\n",
            "Epoch: 20/20... Training loss: 0.0929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jr4WnL_IRmdY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "6a319b11-4b23-4a1d-cfe2-ad933c01e1d7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526899111544,
          "user_tz": -120,
          "elapsed": 1609,
          "user": {
            "displayName": "Matthias Ludwig",
            "photoUrl": "//lh3.googleusercontent.com/-Cacxebueg8s/AAAAAAAAAAI/AAAAAAAAAM8/S7I83k3nq90/s50-c-k-no/photo.jpg",
            "userId": "109901656152479831667"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
        "in_imgs = mnist.test.images[:10]\n",
        "reconstructed, compressed = sess.run([decoded, encoded], feed_dict={inputs_: in_imgs})\n",
        "\n",
        "for images, row in zip([in_imgs, reconstructed], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "fig.tight_layout(pad=0.1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABawAAAEsCAYAAAAvofT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f851df97908>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SFDEqfkiRpXp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sCjNdfgjR-rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convolutional Autoencoders\n",
        "\n",
        "With De-Nosing of images\n",
        "\n",
        "> tf.image.resize_nearest_neighbor() [see Paper](https://distill.pub/2016/deconv-checkerboard/)"
      ]
    },
    {
      "metadata": {
        "id": "wYA7p3epRrMm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
        "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
        "\n",
        "### Encoder\n",
        "conv1 = tf.layers.conv2d(inputs_, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 28x28x32\n",
        "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
        "# Now 14x14x32\n",
        "conv2 = tf.layers.conv2d(maxpool1, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 14x14x32\n",
        "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
        "# Now 7x7x32\n",
        "conv3 = tf.layers.conv2d(maxpool2, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 7x7x16\n",
        "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
        "# Now 4x4x16\n",
        "\n",
        "### Decoder\n",
        "upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))\n",
        "# Now 7x7x16\n",
        "conv4 = tf.layers.conv2d(upsample1, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 7x7x16\n",
        "upsample2 = tf.image.resize_nearest_neighbor(conv4, (14,14))\n",
        "# Now 14x14x16\n",
        "conv5 = tf.layers.conv2d(upsample2, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 14x14x32\n",
        "upsample3 = tf.image.resize_nearest_neighbor(conv5, (28,28))\n",
        "# Now 28x28x32\n",
        "conv6 = tf.layers.conv2d(upsample3, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
        "# Now 28x28x32\n",
        "\n",
        "logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
        "#Now 28x28x1\n",
        "\n",
        "# Pass logits through sigmoid to get reconstructed image\n",
        "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
        "\n",
        "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
        "\n",
        "# Get cost and define the optimizer\n",
        "cost = tf.reduce_mean(loss)\n",
        "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70rnSTa_yvkw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIJPN6qzy60a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "5243a2f4-23cb-4042-9966-da3c08a51b79"
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 200\n",
        "# Set's how much noise we're adding to the MNIST images\n",
        "noise_factor = 0.5\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for e in range(epochs):\n",
        "    for ii in range(mnist.train.num_examples//batch_size):\n",
        "        batch = mnist.train.next_batch(batch_size)\n",
        "        # Get images from the batch\n",
        "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
        "        \n",
        "        # Add random noise to the input images\n",
        "        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n",
        "        # Clip the images to be between 0 and 1\n",
        "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
        "        \n",
        "        # Noisy images as inputs, original images as targets\n",
        "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,\n",
        "                                                         targets_: imgs})\n",
        "        if(ii % 200 == 0):\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Training loss: {:.4f}\".format(batch_cost))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/100... Training loss: 0.6987\n",
            "Epoch: 1/100... Training loss: 0.1790\n",
            "Epoch: 2/100... Training loss: 0.1609\n",
            "Epoch: 2/100... Training loss: 0.1535\n",
            "Epoch: 3/100... Training loss: 0.1422\n",
            "Epoch: 3/100... Training loss: 0.1378\n",
            "Epoch: 4/100... Training loss: 0.1315\n",
            "Epoch: 4/100... Training loss: 0.1317\n",
            "Epoch: 5/100... Training loss: 0.1288\n",
            "Epoch: 5/100... Training loss: 0.1247\n",
            "Epoch: 6/100... Training loss: 0.1204\n",
            "Epoch: 6/100... Training loss: 0.1226\n",
            "Epoch: 7/100... Training loss: 0.1189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3zYG4ury8J8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "### Checking out the performance\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
        "in_imgs = mnist.test.images[:10]\n",
        "noisy_imgs = in_imgs + noise_factor * np.random.randn(*in_imgs.shape)\n",
        "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
        "\n",
        "reconstructed = sess.run(decoded, feed_dict={inputs_: noisy_imgs.reshape((10, 28, 28, 1))})\n",
        "\n",
        "for images, row in zip([noisy_imgs, reconstructed], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "fig.tight_layout(pad=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BGh8jxazYtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 6: Transfer Learning in TensorFlow\n",
        "\n",
        "In practice, you won't typically be training your own huge networks. There are multiple models out there that have been trained for weeks on huge datasets, like [ImageNet](http://www.image-net.org/)\n",
        "\n",
        "[The whole lesson is perfectly reflected in the Transfer_Learning Jupyter Notebook.](https://github.com/matthiasludwig/deep-learning-udacity-material/tree/master/transfer-learning)"
      ]
    },
    {
      "metadata": {
        "id": "D9B8BVrvz1G2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 8: Deep Learning for Cancer Detection with Sebastian Thrun\n",
        "\n",
        "**Sensitivity:** Of all sick people, how many did we diagnose as sick? TP / TP + FN\n",
        "\n",
        "**Specificity:** How many of the healthy people, did we classify as healthy? TN / TN + FP\n",
        "\n",
        "**vs.**\n",
        "\n",
        "**Recall:** Of all the people who have cancer, how many did we diagnose as having cancer? same as Sensitivity\n",
        "\n",
        "**Precision:** Of all the people we diagnosed with cancer, how many actually had cancer? TP / TP + FP\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GKojNhrr1t6J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Receiver Operating Characteristic\n",
        "\n",
        "True Positive Rate = TP / All Positives\n",
        "\n",
        "False Positive Rate = FP / All Negatives\n",
        "\n",
        "The Area under the Perfect ROC Curve is 1. The closer it is to a diagonal (Area around 0.5).\n",
        "\n",
        "Therefore, we can also refer to the ROC curve as the Sensitivity-Specificity Curve.\n",
        "\n",
        "CNNs are more consistent on the confusion matrix."
      ]
    },
    {
      "metadata": {
        "id": "AdCcc1a6F_MV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Networks"
      ]
    },
    {
      "metadata": {
        "id": "0EYSSO9WGCJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 1: Recurrent Neural Networks\n",
        "\n",
        "The neural network architectures so far have been trained using current inputs only. RNNs adress this very basic and important issue by using memory (i.e. past inputs to the network) when producing the current output. => **Temporal Dependencies**\n",
        "\n",
        "Simple RNN: Elman Network\n",
        "\n",
        "**Two Main types of Applications:**\n",
        "1. Classification\n",
        "2. Regression (for example: Time Series Forecasting)\n",
        "\n",
        "**Static Mapping with FFNN:** output only dependent on x and W\n",
        "\n",
        "- Training: Yield a network that generalizes beyond the train set\n",
        "- Evaluation: Test with new data\n",
        "\n",
        "**Activation Function:** Allows the network to represent nonlinear relationshipts between its inputs and outputs. (Most real-life data is non-linear)\n",
        "\n",
        "**Overfitting Problem:**\n",
        "1. Stop the Training process early\n",
        "2. Regularization (Dropout)\n",
        "\n",
        "**Updating the weight once every N steps** = Mini-Batch steps\n",
        "- Reduction of the complexity of the training process\n",
        "- Noise reduction"
      ]
    },
    {
      "metadata": {
        "id": "qkEFdHoy4cao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Recurrent Neural Network\n",
        "\n",
        "**Recurrent** - Ocurring often or repeatedly\n",
        "\n",
        "**Memory Elements** = States\n",
        "\n",
        "Predicting the next word in the sentence is an example for an RNN where it is important to know the last state.\n",
        "\n",
        "**FFNN vs. RNN**\n",
        "\n",
        "The output of the Hidden layer is part of the Input for the next Input.\n",
        "\n",
        "xW = y (FNN - Time does not matter)\n",
        "\n",
        "x,xt-1,xt-n * W = y\n",
        "\n",
        "**Folded model:** In every singel time step the system will look the same\n",
        "\n",
        "Activation Function: xt x Wx + st-1 x Ws\n",
        "\n",
        "Output = softmax(st x Wy)\n",
        "\n",
        "st = activation(xt x Wt + st-1 x Ws)"
      ]
    },
    {
      "metadata": {
        "id": "7rR53mk8AN0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation Through TIme\n",
        "\n",
        "- st = activation(xt wx + st-1 Ws)\n",
        "- yt = activation(st * Wy)\n",
        "- Et = (dt - yt)^2\n",
        "\n",
        "**Gradient calculations needed to adjust Wy**\n",
        "\n",
        "partialEn/partialWn = partialEn / partial yn * partial yn / partial Wy\n",
        "\n",
        "**Gradient calculations needed to adjust Ws**\n",
        "\n",
        "In BPTT we will take into account every gradient semming frome each state, accumulating all of these contributions."
      ]
    },
    {
      "metadata": {
        "id": "hCZdh3rMIBHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### From RNN to LSTM\n",
        "\n",
        "--> To adress the vanishing Gradient Problem\n",
        "\n",
        "Exploding Gradient Problem = Gradient grows uncontrollably.\n",
        "- Penalizing large gradients. (Threshold)\n",
        "\n",
        "LSTM Cell:\n",
        "- Input can be stored without being forgotten.\n",
        "- Avoid the loss of information due to the Vanishing Gradient Problem\n",
        "\n",
        "Four seperate calcualtions:\n",
        "- Fully differentiable\n",
        "- Sigmoid (3x)\n",
        "- Hyperbolic tangent\n",
        "- Multiplication\n",
        "- Addition\n"
      ]
    },
    {
      "metadata": {
        "id": "Y3Kslk12KaIo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 2: Long Short-Term Memory Networks\n",
        "\n",
        "Long Term Memory:\n",
        "- Show about Nature and Science\n",
        "- Lots of forest Animals\n",
        "\n",
        "Short Term Memory:\n",
        "- Squirrels\n",
        "- Trees\n",
        "\n",
        "Event:\n",
        "- Dog/Wolf\n",
        "\n",
        "Output:\n",
        "- Wolf (0.8)\n",
        "\n",
        "Than update the Long Term Memory and update the Short Term Memory.\n",
        "\n",
        "**Forget Gate**\n",
        "LT forgets everything it doesnt consider useful\n",
        "\n",
        "**Learn Gate**\n",
        "ST + Event\n",
        "\n",
        "**Remember Gate**\n",
        "LT + ST + Event => New Long Term Memory\n",
        "\n",
        "**Use Gate**\n",
        "\n",
        "LT + ST + EVent => New Short Term Memory AND Output"
      ]
    },
    {
      "metadata": {
        "id": "LNK3UqqOTJMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learn Gate\n",
        "\n",
        "*Combine and Ignore*\n",
        "\n",
        "Nt = tanh(Wn[ST Mt-1, Et] + bn) x it\n",
        "\n",
        "it is the Ignore Factor.\n",
        "\n",
        "it = sigmoid(Wi[ST Mt=1, Et] + bj)"
      ]
    },
    {
      "metadata": {
        "id": "nPUkwTbMTzhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forget Gate\n",
        "\n",
        "*Keep and Forget*\n",
        "\n",
        "LT Mt-1 * ft\n",
        "\n",
        "ft = sigmoid(Wf [ST Mt-1, Et] + bf)"
      ]
    },
    {
      "metadata": {
        "id": "WGSqX0CjUM1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remember Gate\n",
        "\n",
        "Add Forget Gate and Learn Gate\n",
        "\n",
        "LT M t-1 ft + Ntit"
      ]
    },
    {
      "metadata": {
        "id": "rTwN1lgxUcAW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Gate\n",
        "\n",
        "Ut = tanh(Wu LT Mt-1 * ft + bu)\n",
        "\n",
        "Vt = sigmoid(Wv [ST Mt-1, Et] + bv)\n",
        "\n",
        "Ut x Vt"
      ]
    },
    {
      "metadata": {
        "id": "a0pv87jvVXQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Other architectures\n",
        "\n",
        "- Gated Recurrent Unit (GRU)\n",
        "- Peephole Connections (LT Mt-1 into the forget factor)"
      ]
    },
    {
      "metadata": {
        "id": "BI5I6StchVy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lesson 3: Implementation of RNN and LSTM\n",
        "\n",
        "**Character-wise RNN:** A Network will learn about text one character at a time and then generate new text.\n",
        "\n",
        "**Getting the batches right:**\n",
        "- RNNs: Sequences of data. Splitting Sequences in shorter sequences.\n",
        "- Batch size: Number of sequences\n",
        "- Sequence lenght: How much of each sequence is inputted.\n"
      ]
    },
    {
      "metadata": {
        "id": "0cEs7EsiTIeG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}